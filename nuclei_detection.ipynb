{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### it may be necessary to restart the kernel after installation (for whatever reason)\n",
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "import torch, torchvision\n",
    "if torch.version.cuda == '10.0.130':\n",
    "    !{sys.executable} -m pip install --upgrade pip\n",
    "    !{sys.executable} -m pip install -U torch==1.4+cu100 torchvision==0.5+cu100 -f https://download.pytorch.org/whl/torch_stable.html \n",
    "    !{sys.executable} -m pip install cython pyyaml==5.1\n",
    "    !{sys.executable} -m pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
    "    !{sys.executable} -m pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu100/index.html\n",
    "elif torch.version.cuda == '10.1':\n",
    "    !{sys.executable} -m pip install cython \n",
    "    !{sys.executable} -m pip install pyyaml\n",
    "    !{sys.executable} -m pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
    "    !{sys.executable} -m pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html\n",
    "else:\n",
    "    print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append('/opt/conda/lib/python3.7/site-packages')\n",
    "#sys.path.append('/opt/conda/envs/DLM_R2Py/lib/python3.7/site-packages')\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "#!{sys.executable} -m pip install --upgrade matplotlib\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "#from bokeh.io import output_notebook\n",
    "\n",
    "from typing import Dict, List, Callable, Tuple, Union\n",
    "\n",
    "\n",
    "sys.path.append('../wsi_processing_pipeline/')\n",
    "sys.path.append('../wsi_processing_pipeline/tile_extraction/')\n",
    "from tile_extraction import tiles, util, slide\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "Path.ls = lambda x: [p for p in list(x.iterdir()) if '.ipynb_checkpoints' not in p.name]\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "import fastai\n",
    "from fastai import vision\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches\n",
    "import pycocotools\n",
    "#from pycocotools import mask\n",
    "from itertools import groupby\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "import random\n",
    "import scipy.io\n",
    "import json\n",
    "import pickle\n",
    "import math\n",
    "import multiprocessing\n",
    "\n",
    "#Base path to datasets\n",
    "PATH = Path('/home/Deep_Learner/private/datasets/nuclei_datasets/')\n",
    "\n",
    "\n",
    "dataset_name_train = 'nuclei_train'\n",
    "dataset_name_valid = 'nuclei_valid'\n",
    "dataset_name_test = 'nuclei_test'\n",
    "dataset_name_all_combined = 'all_combined'\n",
    "dataset_name_all_combined_without_excluded = 'all_combined_without_excluded'\n",
    "\n",
    "seed = 19\n",
    "np.random.seed(seed)\n",
    "\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.utils.visualizer import ColorMode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# util functions  // TODO: function from polygon representation to binary mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(dataset_dicts:List[Dict],\n",
    "              metadata:detectron2.data.catalog.Metadata,\n",
    "              ground_truth_available:bool,\n",
    "              with_preds:bool,\n",
    "              predictor, \n",
    "              extract_otf_from_wsi:bool=False, \n",
    "              captions:List[str] = None, \n",
    "              figsize=(20,20), \n",
    "              caption_generators:List[Callable]=None):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        caption_generators: every caption_generator is a function that takes a dict from the dataset_dicts list and returns\n",
    "                            some value, which will be printed together with the function name over the image\n",
    "    \"\"\"\n",
    "    for n, d in enumerate(dataset_dicts):\n",
    "        path = d['file_name']\n",
    "        print(path)\n",
    "                \n",
    "        imgs_to_show = []\n",
    "               \n",
    "        ### original image\n",
    "        img = None\n",
    "        if not extract_otf_from_wsi:\n",
    "            ###some of the images are in rgba format, [:,:,:3] drops the alpha\n",
    "            img = numpy.array(PIL.Image.open(path))[:,:,:3]\n",
    "        else:\n",
    "            img = numpy.array(open_image_from_wsi(d))           \n",
    "        imgs_to_show.append(img)\n",
    "        \n",
    "        ### instantiate Visualizer\n",
    "        v = Visualizer(img,\n",
    "                       metadata=metadata, \n",
    "                       scale=1, \n",
    "                       instance_mode=ColorMode.IMAGE)\n",
    "        \n",
    "        ### prediction\n",
    "        if with_preds:\n",
    "            pred = predictor(img[:, :, ::-1])\n",
    "            n_predicted = len(pred['instances'])\n",
    "            print(f'number of predicted instances: {n_predicted}')\n",
    "            img_with_preds = v.draw_instance_predictions(pred[\"instances\"].to(\"cpu\")).get_image()\n",
    "            imgs_to_show.append(img_with_preds)\n",
    "            \n",
    "                                \n",
    "        ### ground truth\n",
    "        if ground_truth_available:\n",
    "            ### image with ground truth boxes and masks   \n",
    "            img_with_ground_truth = v.draw_dataset_dict(d).get_image()\n",
    "            imgs_to_show.append(img_with_ground_truth)\n",
    "            \n",
    "            ##  // TODO: function from polygon representation to binary mask\n",
    "            ### merged masks of the ground truth\n",
    "            #masks = None\n",
    "            #try:\n",
    "            #    masks = [open_mask_as_np(mp) for mp in get_masks_path_from_id(path.stem, base_path=path.parent.parent.parent).ls()]    \n",
    "            #except FileNotFoundError:\n",
    "            #    masks = get_mask_list_from_rle_for_one_id(path.stem, df_rle_labels, shape=(d['width'],d['height']))\n",
    "            #n_actual = d['annotations']\n",
    "            #print(f'number of actual instances: {len(n_actual)}')\n",
    "            #print(f'number of actual instances: {len(masks)}')\n",
    "            #merged_mask_ground_truth = merge_masks(masks, shape=(masks[0].shape[0], masks[0].shape[1]))\n",
    "            #imgs_to_show.append(merged_mask_ground_truth)\n",
    "            \n",
    "        \n",
    "        ### captions\n",
    "        try:\n",
    "            print(captions[n])\n",
    "        except:\n",
    "            ## do nothing\n",
    "            pass\n",
    "        \n",
    "        if caption_generators != None and len(caption_generators) > 0:\n",
    "            for cg in caption_generators:\n",
    "                try:\n",
    "                    print(f'{cg.__name__}: {cg(d)}')\n",
    "                except:\n",
    "                    print(f'{cg.__name__}: failed')\n",
    "               \n",
    "        ### plot    \n",
    "        f, axarr = plt.subplots(1,len(imgs_to_show), figsize=figsize)\n",
    "        for n, i in enumerate(imgs_to_show):\n",
    "            if len(i.shape) == 2:\n",
    "                axarr[n].imshow(i)\n",
    "            else:\n",
    "                axarr[n].imshow(i)            \n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def get_all_file_paths_recursively(path:pathlib.Path, suffix:str = '.png')->List[pathlib.Path]:\n",
    "    paths = []\n",
    "    for p in path.ls():\n",
    "        if p.is_file() and p.suffix == suffix:\n",
    "            paths.append(p)\n",
    "        elif p.is_dir():\n",
    "            paths = paths + get_all_file_paths_recursively(p)\n",
    "    return paths \n",
    "\n",
    "def save_as_pickle(obj:object, path):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "\n",
    "def save_as_json(dataset_dict, path):\n",
    "    with open(path, 'w') as outfile:\n",
    "        json.dump(dataset_dict, outfile)\n",
    "\n",
    "def load_json(path)->List[Dict]:\n",
    "    with open(path) as json_file:\n",
    "        #return json.loads(json.load(json_file))\n",
    "        return json.load(json_file)\n",
    "\n",
    "\n",
    "    \n",
    "from matplotlib.pyplot import imshow\n",
    "def show_pil(path:pathlib.Path):    \n",
    "    imshow(np.asarray(Image.open(path)))\n",
    "    plt.show()\n",
    "\n",
    "def get_path_from_id(img_id:str, base_path:pathlib.Path)->pathlib.Path:\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        base_path: here ~/2018_Data_Science_Bowl/stage1_train/   or   ~/2018_Data_Science_Bowl/stage1_test/\n",
    "    \"\"\"\n",
    "    for p in base_path.ls():\n",
    "        if img_id in p.stem:\n",
    "            return p\n",
    "\n",
    "def get_masks_path_from_id(img_id:str, base_path:pathlib.Path)->pathlib.Path:\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        base_path: here ~/2018_Data_Science_Bowl/stage1_train/   or   ~/2018_Data_Science_Bowl/stage1_test/\n",
    "    \"\"\"\n",
    "    return get_path_from_id(img_id, base_path)/'masks'\n",
    "        \n",
    "def open_mask_as_np(path:pathlib.Path)->numpy.ndarray:\n",
    "    return np.asarray(PIL.Image.open(path), dtype=np.bool)\n",
    "\n",
    "\n",
    "def show_np(arr:numpy.ndarray, figsize:tuple=(4,4)):\n",
    "    #plt.imshow(arr)\n",
    "    #plt.show()\n",
    "    f, axarr = plt.subplots(1,1, figsize=figsize)\n",
    "    axarr.imshow(arr)            \n",
    "    plt.show()\n",
    "\n",
    "def show_np_with_bboxes(img:numpy.ndarray, bboxes:List[numpy.ndarray], figsize:tuple=(10,10)):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        img: img as numpy array\n",
    "        bboxes: List of bounding boxes where each bbox is a numpy array: \n",
    "                array([ x-upper-left, y-upper-left,  width,  height]) \n",
    "                e.g. array([ 50., 211.,  17.,  19.])\n",
    "    \"\"\"    \n",
    "    # Create figure and axes\n",
    "    fig,ax = plt.subplots(1,1,figsize=figsize)    \n",
    "    # Display the image\n",
    "    ax.imshow(img)    \n",
    "    # Create a Rectangle patch for each bbox\n",
    "    for b in bboxes:\n",
    "        rect = matplotlib.patches.Rectangle((b[0],b[1]),b[2],b[3],linewidth=1,edgecolor='r',facecolor='none')    \n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)    \n",
    "    plt.show() \n",
    "  \n",
    "    \n",
    "def merged_mask_to_list_of_single_masks(merged_mask:numpy.array, \n",
    "                                        noise_removal:bool=False, \n",
    "                                        distance_transform_threshold:float=0.0 )->List[numpy.array]:\n",
    "    \"\"\"\n",
    "    techniques from here:\n",
    "    https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_watershed/py_watershed.html\n",
    "    \n",
    "        Arguments:\n",
    "           merged_mask: a binary mask as numpy array with shape (height, width), no third dimension\n",
    "           noise_removal: if True, small noise will be removed\n",
    "           distance_transform_threshold: between 0.0 and 1.0, the larger the more \"shrinking\" will be applied to seperate\n",
    "                                           touching objects via distance transformation\n",
    "        Result:\n",
    "            List of binary mask for every single instance\n",
    "    \"\"\"\n",
    "    # noise removal\n",
    "    if noise_removal:\n",
    "        kernel = np.ones((3,3),np.uint8)\n",
    "        merged_mask = cv2.morphologyEx(merged_mask,cv2.MORPH_OPEN,kernel, iterations = 2)\n",
    "    \n",
    "    # separate touching objects via shrinking\n",
    "    if distance_transform_threshold > 0.0:\n",
    "        merged_mask = cv2.distanceTransform(merged_mask,cv2.DIST_L2,5)\n",
    "        ret, merged_mask = cv2.threshold(merged_mask,distance_transform_threshold*merged_mask.max(),255,0)\n",
    "        merged_mask = merged_mask.astype(np.uint8)\n",
    "\n",
    "    ret, markers = cv2.connectedComponents(merged_mask)\n",
    "    return [(markers == n).astype(int) for n in range(1, ret)]\n",
    "\n",
    "    \n",
    "    \n",
    "def merge_masks(masks:List[numpy.ndarray], shape:tuple=(256,256))->numpy.ndarray:\n",
    "    merged_mask = np.zeros((shape[0],shape[1]), dtype=np.bool)\n",
    "    for mask in masks:\n",
    "        merged_mask = np.maximum(merged_mask, mask)\n",
    "    return merged_mask\n",
    "\n",
    "def merge_masks_from_path(path:pathlib.Path, shape:tuple=(256,256))->numpy.ndarray:\n",
    "    return merge_masks([open_mask_as_np(p) for p in path.ls()], shape)\n",
    "\n",
    "\n",
    "def rle_encode(mask_np:numpy.ndarray)->List[int]:\n",
    "    '''\n",
    "    mask_np: numpy array of shape (height, width), 1 = mask, 0 = background\n",
    "    Returns run length as list\n",
    "    '''\n",
    "    dots = np.where(mask_np.T.flatten()==1)[0] # .T sets Fortran order down-then-right\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b>prev+1): run_lengths.extend((b+1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "def rle_from_list_to_string(rle_as_list:List[int])->str:\n",
    "    return ' '.join([str(e) for e in rle_as_list])\n",
    "\n",
    "def rle_decode(mask_rle:str, shape:tuple=(256, 256))->numpy.ndarray:\n",
    "    '''\n",
    "    Arguments:\n",
    "        mask_rle: run-length as string formated (start length) e.g. \"6908 1 7161 8 7417 8\"\n",
    "        shape: (height,width) of array to return\n",
    "    Returns: \n",
    "        numpy array, True == mask, False == background\n",
    "    '''\n",
    "    #print('rle_decode(mask_rle = ', mask_rle)\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    mask = np.zeros(shape[0]*shape[1], dtype=np.bool)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        mask[lo:hi] = True\n",
    "    # Needed to align to RLE direction\n",
    "    return mask.reshape(shape).T\n",
    "\n",
    "def get_mask_list_from_rle_for_one_id(img_id:str, \n",
    "                                      df:pandas.DataFrame, \n",
    "                                      coloumn_name_ids:str='ImageId',\n",
    "                                      coloumn_name_rle:str='EncodedPixels',\n",
    "                                      shape:tuple=(256,256))->List[numpy.ndarray]:\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        shape: tuple (width, height)\n",
    "    Returns:\n",
    "        \n",
    "    \"\"\"\n",
    "    masks_rle_df = df.loc[df[coloumn_name_ids] == img_id]\n",
    "    masks = []\n",
    "    for index, row in masks_rle_df.iterrows():\n",
    "        #print(row[coloumn_name_rle])\n",
    "        masks.append(rle_decode(row[coloumn_name_rle], shape))\n",
    "    return masks\n",
    "\n",
    "def mask_to_bbox(mask_np:numpy.ndarray)->numpy.ndarray:\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        mask_np: binary mask as numpy array where mask == True or 1 or 1.0 and background == False or 0 or 0.0\n",
    "    Returns:\n",
    "        bounding box as numpy array: array([ x-upper-left, y-upper-left,  width,  height]) \n",
    "                                        e.g. array([ 50., 211.,  17.,  19.])\n",
    "    \"\"\"\n",
    "    return pycocotools.mask.toBbox(pycocotools.mask.encode(np.asarray(mask_np, order=\"F\")))\n",
    "\n",
    "\n",
    "def rle_encode_pycoco(mask_np:numpy.ndarray)->dict:\n",
    "    '''\n",
    "    Arguments:\n",
    "        mask_np: numpy array of shape (height, width), 1 = mask, 0 = background\n",
    "    Returns: \n",
    "        dict with size and per-pixel segmentation mask in COCO's RLE format.        \n",
    "    '''\n",
    "    #option 1\n",
    "    return pycocotools.mask.encode(np.asarray(mask, order=\"F\"))\n",
    "    #option 2\n",
    "    #rle = {'counts': [], 'size': list(mask_np.shape)}\n",
    "    #counts = rle.get('counts')\n",
    "    #for i, (value, elements) in enumerate(groupby(mask_np.ravel(order='F'))):\n",
    "    #    if i == 0 and value == 1:\n",
    "    #        counts.append(0)\n",
    "    #    counts.append(len(list(elements)))\n",
    "    #return rle\n",
    "\n",
    "#!{sys.executable} -m pip install imantics\n",
    "import imantics\n",
    "from imantics import Polygons, Mask\n",
    "def get_polygon_from_binary_mask(mask_np:numpy.ndarray)->List[float]:\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        binary mask as numpy array\n",
    "    Returns:\n",
    "        list[int] is one simple polygon in the format of [x1, y1, ..., xn, yn]\n",
    "    \"\"\"\n",
    "    return [float(c) for c in list(Mask(mask_np).polygons()[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# some util examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## load csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "s1_trn_lbs_df = pd.read_csv(STAGE1_TRAIN_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "s1_trn_lbs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img_id = s1_trn_lbs_df.iloc[0][0];img_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## merge existing single masks of one image - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img_id_p = get_path_from_id(img_id, STAGE1_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "masks_path = get_masks_path_from_id(img_id, STAGE1_TRAIN);masks_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "masks_np = [open_mask_as_np(p) for p in masks_path.ls()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "merged_mask = merge_masks_from_path(masks_path,(256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show_np(merged_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## decode run length encoding to mask - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "masks = get_mask_list_from_rle_for_one_id(img_id, s1_trn_lbs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "merged_mask = merge_masks(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show_np(merged_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## get bounding box from mask in format x,y (left upper corner); width, heigth - example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "img_p = get_path_from_id(img_id, STAGE1_TRAIN);img_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "masks_p = get_masks_path_from_id(img_id, STAGE1_TRAIN);masks_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bboxes = []\n",
    "for p in masks_p.ls():\n",
    "    mask_np = open_mask_as_np(p)\n",
    "    bboxes.append(mask_to_bbox(mask_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "merged_mask = merge_masks_from_path(masks_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show_np_with_bboxes(merged_mask,bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Get various datsets into right format for detectron2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### dsb 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#data science bowl 2018 dataset\n",
    "DSB_2018 = PATH/'2018_Data_Science_Bowl'\n",
    "DSB_2018_STAGE1_TRAIN = DSB_2018/'stage1_train'\n",
    "DSB_2018_STAGE1_TEST = DSB_2018/'stage1_test'\n",
    "DSB_2018_STAGE1_TRAIN_LABELS = DSB_2018/'stage1_train_labels.csv'\n",
    "DSB_2018_STAGE1_TEST_LABELS = DSB_2018/'stage1_solution.csv'\n",
    "DSB_2018_STAGE2_TEST = DSB_2018/'stage2_test_final'\n",
    "DSB_2018_STAGE2_TEST_SAMPLE_SUBMISSION = DSB_2018/'stage2_sample_submission_final.csv'\n",
    "\n",
    "dataset_name_dsb18_train_and_test1 = 'dsb18_train_and_test1'\n",
    "dataset_name_dsb18_test2 = 'dsb18_test2'\n",
    "\n",
    "def get_nuclei_dicts_dsb18(isTestSet:bool=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \"\"\"\n",
    "    dataset_dicts_dsb18_pickle_filepath = None\n",
    "    if isTestSet:        \n",
    "        dataset_dicts_dsb18_pickle_filepath = Path('./dataset_dicts/dataset_dicts_dsb18_test2.pickle')\n",
    "    else:\n",
    "        dataset_dicts_dsb18_pickle_filepath = Path('./dataset_dicts/dataset_dicts_dsb18_train+test1.pickle')\n",
    "        \n",
    "    if dataset_dicts_dsb18_pickle_filepath.exists():\n",
    "        return load_pickle(dataset_dicts_dsb18_pickle_filepath)\n",
    "    else:        \n",
    "        img_paths = None\n",
    "        if isTestSet:\n",
    "            img_paths = [path/'images'/f'{path.name}.png' for path in DSB_2018_STAGE2_TEST.ls()]\n",
    "        else:\n",
    "            excluded_ids = ['7b38c9173ebe69b4c6ba7e703c0c27f39305d9b2910f46405993d2ea7a963b80']\n",
    "            img_paths = [p/'images'/f'{p.name}.png' for p in DSB_2018_STAGE1_TRAIN.ls()+DSB_2018_STAGE1_TEST.ls() \\\n",
    "                 if p.name not in excluded_ids]\n",
    "                   \n",
    "        dataset_dicts = []\n",
    "        for path in tqdm(img_paths):\n",
    "            path = Path(path)\n",
    "            record = {}\n",
    "    \n",
    "            img_pil = PIL.Image.open(path)\n",
    "                \n",
    "            width = img_pil.width\n",
    "            height = img_pil.height\n",
    "            \n",
    "            record[\"file_name\"] = str(path)\n",
    "            record[\"image_id\"] = path.stem\n",
    "            record[\"height\"] = height\n",
    "            record[\"width\"] = width\n",
    "            \n",
    "            \n",
    "            objs = []\n",
    "            if not isTestSet:\n",
    "                df_rle_labels = pd.concat([pd.read_csv(DSB_2018_STAGE1_TRAIN_LABELS), \n",
    "                                pd.read_csv(DSB_2018_STAGE1_TEST_LABELS)], \n",
    "                                ignore_index=True, sort=False)\n",
    "                masks = get_mask_list_from_rle_for_one_id(path.stem, df_rle_labels, shape=(width,height))\n",
    "                                \n",
    "                for mask in masks:\n",
    "                    obj = {\"bbox\": list(mask_to_bbox(mask)),\n",
    "                            \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                            \"segmentation\": [get_polygon_from_binary_mask(mask)],\n",
    "                            #\"segmentation\": rle_encode_pycoco(mask),\n",
    "                            \"category_id\": 0,\n",
    "                            \"iscrowd\": 0}\n",
    "                    ### detectron2 lib throws AssertionError if not (len(polygon) % 2 == 0 and len(polygon) >= 6)\n",
    "                    if len(obj[\"segmentation\"][0]) >= 6:\n",
    "                        objs.append(obj)\n",
    "                    \n",
    "            record[\"annotations\"] = objs\n",
    "            dataset_dicts.append(record)\n",
    "        save_as_pickle(dataset_dicts, dataset_dicts_dsb18_pickle_filepath)\n",
    "        return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_dicts_dsb18_train_and_test1 = get_nuclei_dicts_dsb18(isTestSet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_dicts_dsb18_test2 = get_nuclei_dicts_dsb18(isTestSet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DatasetCatalog.clear()\n",
    "DatasetCatalog.register(dataset_name_dsb18_train_and_test1, lambda : get_nuclei_dicts_dsb18_train_and_test1)\n",
    "MetadataCatalog.get(dataset_name_dsb18_train_and_test1).set(thing_classes=[\"nucleus\"])\n",
    "metadata_nuclei_dsb18_train_and_test1 = MetadataCatalog.get(dataset_name_dsb18_train_and_test1)\n",
    "\n",
    "visualize(random.sample(dataset_dicts_dsb18_train_and_test1,4), \n",
    "          metadata_nuclei_dsb18_train_and_test1,\n",
    "             ground_truth_available=True,\n",
    "             with_preds=False, \n",
    "             predictor=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### MoNuSeg 2018 (TODO: test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### MoNuSeg 2018\n",
    "MONUSEG_2018 = PATH/'MONUSEG_2018'\n",
    "MONUSEG_2018_TRAIN = MONUSEG_2018/'MoNuSeg_2018_Training_data'\n",
    "MONUSEG_2018_TEST = MONUSEG_2018/'MoNuSegTestData'\n",
    "\n",
    "dataset_name_monuseg18 = 'monuseg18'\n",
    "\n",
    "def get_nuclei_dicts_monuseg18():\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \"\"\"    \n",
    "    dataset_dicts_monuseg18_pickle_filepath = Path('./dataset_dicts/dataset_dicts_monuseg18.pickle')\n",
    "        \n",
    "    if dataset_dicts_monuseg18_pickle_filepath.exists():\n",
    "        return load_pickle(dataset_dicts_monuseg18_pickle_filepath)\n",
    "    else:        \n",
    "        img_paths_train = [p/'images'/f'{p.name}.png' for p in MONUSEG_2018_TRAIN.ls() if not '.ipynb_checkpoints' in str(p)] \n",
    "        img_paths_test = [p for p in MONUSEG_2018_TEST.ls() if p.suffix == '.tif']        \n",
    "        \n",
    "        dataset_dicts = []\n",
    "        for path in tqdm(img_paths_train):\n",
    "            path = Path(path)\n",
    "            record = {}\n",
    "    \n",
    "            img_pil = PIL.Image.open(path)\n",
    "                \n",
    "            record[\"file_name\"] = path\n",
    "            record[\"image_id\"] = path.stem\n",
    "            record[\"height\"] = img_pil.height\n",
    "            record[\"width\"] = img_pil.width\n",
    "                        \n",
    "            masks = [open_mask_as_np(mp) for mp in \\\n",
    "                     get_masks_path_from_id(path.stem, base_path=path.parent.parent.parent).ls()]    \n",
    "            objs = []                    \n",
    "            for mask in masks:\n",
    "                obj = {\"bbox\": list(mask_to_bbox(mask)),\n",
    "                        \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                        \"segmentation\": [get_polygon_from_binary_mask(mask)],\n",
    "                        #\"segmentation\": rle_encode_pycoco(mask),\n",
    "                        \"category_id\": 0,\n",
    "                        \"iscrowd\": 0}\n",
    "                ### detectron2 lib throws AssertionError if not (len(polygon) % 2 == 0 and len(polygon) >= 6)\n",
    "                if len(obj[\"segmentation\"][0]) >= 6:\n",
    "                    objs.append(obj)\n",
    "                    \n",
    "            record[\"annotations\"] = objs\n",
    "            dataset_dicts.append(record)\n",
    "            \n",
    "        #for path in tqdm(img_paths_test):\n",
    "            ###\n",
    "            # TODO\n",
    "            ###\n",
    "            \n",
    "        save_as_pickle(dataset_dicts, dataset_dicts_monuseg18_pickle_filepath)\n",
    "        return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_dicts_monuseg18 = get_nuclei_dicts_monuseg18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DatasetCatalog.clear()\n",
    "DatasetCatalog.register(dataset_name_monuseg18, lambda : get_nuclei_dicts_monuseg18)\n",
    "MetadataCatalog.get(dataset_name_monuseg18).set(thing_classes=[\"nucleus\"])\n",
    "metadata_nuclei_monuseg18 = MetadataCatalog.get(dataset_name_monuseg18)\n",
    "\n",
    "visualize(random.sample(dataset_dicts_monuseg18,4), \n",
    "          metadata_nuclei_monuseg18,\n",
    "             ground_truth_available=True,\n",
    "             with_preds=False, \n",
    "             predictor=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### CoNSeP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### CoNSeP dataset\n",
    "CONSEP = PATH/'CoNSeP'\n",
    "CONSEP_IMAGES = CONSEP/'Images'\n",
    "CONSEP_LABELS = CONSEP/'Labels'\n",
    "\n",
    "dataset_name_consep = 'consep'\n",
    "\n",
    "def get_nuclei_dicts_consep():\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \"\"\" \n",
    "    dataset_dicts_consep_pickle_filepath = Path('./dataset_dicts/dataset_dicts_consep.pickle')\n",
    "    if dataset_dicts_consep_pickle_filepath.exists():\n",
    "        return load_pickle(dataset_dicts_consep_pickle_filepath)\n",
    "    else:\n",
    "        consep_image_paths = CONSEP_IMAGES.ls()\n",
    "        consep_label_paths = CONSEP_LABELS.ls()\n",
    "        dataset_dicts = []\n",
    "        \n",
    "        for img_path in tqdm(consep_image_paths):\n",
    "            record = {}\n",
    "            \n",
    "            name = img_path.stem\n",
    "            ### find corresponding .mat label file for image\n",
    "            label_path = None\n",
    "            for lbl_path in consep_label_paths:\n",
    "                if name == lbl_path.stem:\n",
    "                    label_path = lbl_path\n",
    "                    break\n",
    "            ### every instance mask is encoded with a unique number, all in one channel\n",
    "            masks_combined = scipy.io.loadmat(label_path)['inst_map']\n",
    "            \n",
    "            height = masks_combined.shape[0]\n",
    "            width = masks_combined.shape[1]\n",
    "                    \n",
    "            record[\"file_name\"] = img_path\n",
    "            record[\"image_id\"] = img_path\n",
    "            record[\"height\"] = height\n",
    "            record[\"width\"] = width\n",
    "                   \n",
    "            masks = [(masks_combined == n).astype(int) for n in range(1, int(np.max(masks_combined))+1)]    \n",
    "            \n",
    "            objs = []\n",
    "            for mask in masks:\n",
    "                obj = {\"bbox\": list(mask_to_bbox(mask.astype(np.uint8))),\n",
    "                        \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                        \"segmentation\": [get_polygon_from_binary_mask(mask)],\n",
    "                        #\"segmentation\": rle_encode_pycoco(mask),\n",
    "                        \"category_id\": 0,\n",
    "                        \"iscrowd\": 0}\n",
    "                ### detectron2 lib throws AssertionError if not (len(polygon) % 2 == 0 and len(polygon) >= 6)\n",
    "                if len(obj[\"segmentation\"][0]) >= 6:\n",
    "                    objs.append(obj)\n",
    "                    \n",
    "            # free memory, otherwise kernel will die\n",
    "            masks = None        \n",
    "            record[\"annotations\"] = objs\n",
    "            dataset_dicts.append(record)\n",
    "            \n",
    "        save_as_pickle(dataset_dicts, dataset_dicts_consep_pickle_filepath)    \n",
    "        return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_dicts_consep = get_nuclei_dicts_consep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DatasetCatalog.clear()\n",
    "DatasetCatalog.register(dataset_name_consep, lambda : get_nuclei_dicts_consep)\n",
    "MetadataCatalog.get(dataset_name_consep).set(thing_classes=[\"nucleus\"])\n",
    "metadata_nuclei_consep = MetadataCatalog.get(dataset_name_consep)\n",
    "\n",
    "visualize(random.sample(dataset_dicts_consep,4), \n",
    "          metadata_nuclei_consep,\n",
    "             ground_truth_available=True,\n",
    "             with_preds=False, \n",
    "             predictor=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### janowczyk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# janowczyk http://www.andrewjanowczyk.com/use-case-1-nuclei-segmentation/\n",
    "JANOWCZYK = PATH/'janowczyk'\n",
    "\n",
    "dataset_name_janowczyk = 'janowczyk'\n",
    "\n",
    "def janowczyk_img_name_to_id(img_name:str):\n",
    "    splits = img_name.split('_')\n",
    "    return f'{splits[0]}_{splits[1]}_{splits[2]}'\n",
    "\n",
    "\n",
    "def janowczyk_find_mask_for_image(img_path, mask_paths:List[pathlib.Path]):\n",
    "    for mask_p in mask_paths:\n",
    "        if janowczyk_img_name_to_id(img_path.name) == janowczyk_img_name_to_id(mask_p.name):\n",
    "            return mask_p\n",
    "\n",
    "def get_nuclei_dicts_janowczyk():\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \"\"\" \n",
    "    dataset_dicts_janowczyk_pickle_filepath = Path('./dataset_dicts/dataset_dicts_janowczyk.pickle')\n",
    "    if dataset_dicts_janowczyk_pickle_filepath.exists():\n",
    "        return load_pickle(dataset_dicts_janowczyk_pickle_filepath)\n",
    "    else:\n",
    "        janowczyk_image_paths = [p for p in JANOWCZYK.ls() if p.suffix == '.tif'] \n",
    "        janowczyk_mask_paths = [p for p in JANOWCZYK.ls() if p.suffix == '.png']\n",
    "        \n",
    "        dataset_dicts = []       \n",
    "        for img_path in tqdm(janowczyk_image_paths):\n",
    "            record = {}\n",
    "            \n",
    "            mask_path = janowczyk_find_mask_for_image(img_path, janowczyk_mask_paths)\n",
    "            mask_merged = cv2.imread(str(mask_path))[:,:,0]\n",
    "                               \n",
    "            record[\"file_name\"] = img_path\n",
    "            record[\"image_id\"] = janowczyk_img_name_to_id(img_path.name)\n",
    "            record[\"height\"] = mask_merged.shape[0]\n",
    "            record[\"width\"] = mask_merged.shape[1]\n",
    "                   \n",
    "            masks = merged_mask_to_list_of_single_masks(merged_mask=mask_merged, \n",
    "                                                        noise_removal=True, \n",
    "                                                        distance_transform_threshold=0.0)    \n",
    "            \n",
    "            objs = []\n",
    "            for mask in masks:\n",
    "                obj = {\"bbox\": list(mask_to_bbox(mask.astype(np.uint8))),\n",
    "                        \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                        \"segmentation\": [get_polygon_from_binary_mask(mask)],\n",
    "                        #\"segmentation\": rle_encode_pycoco(mask),\n",
    "                        \"category_id\": 0,\n",
    "                        \"iscrowd\": 0}\n",
    "                ### detectron2 lib throws AssertionError if not (len(polygon) % 2 == 0 and len(polygon) >= 6)\n",
    "                if len(obj[\"segmentation\"][0]) >= 6:\n",
    "                    objs.append(obj)\n",
    "                    \n",
    "            # free memory, otherwise kernel will die\n",
    "            masks = None\n",
    "            mask_merged = None\n",
    "            record[\"annotations\"] = objs\n",
    "            dataset_dicts.append(record)\n",
    "            \n",
    "        save_as_pickle(dataset_dicts, dataset_dicts_janowczyk_pickle_filepath)    \n",
    "        return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_dicts_janowczyk = get_nuclei_dicts_janowczyk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DatasetCatalog.clear()\n",
    "DatasetCatalog.register(dataset_name_janowczyk, lambda : get_nuclei_dicts_janowczyk)\n",
    "MetadataCatalog.get(dataset_name_janowczyk).set(thing_classes=[\"nucleus\"])\n",
    "metadata_nuclei_janowczyk = MetadataCatalog.get(dataset_name_janowczyk)\n",
    "\n",
    "visualize(random.sample(dataset_dicts_janowczyk,4), \n",
    "          metadata_nuclei_janowczyk,\n",
    "         ground_truth_available=True,\n",
    "         with_preds=False, \n",
    "         predictor=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### TNBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "TNBC = PATH/'TNBC_NucleiSegmentation'\n",
    "TNBC_SLIDES = TNBC/'slides'\n",
    "TNBC_MASKS = TNBC/'masks'\n",
    "dataset_name_tnbc = 'tnbc'\n",
    "\n",
    "def tnbc_find_mask_for_image(img_path, mask_paths:List[pathlib.Path]):\n",
    "    for mask_p in mask_paths:\n",
    "        if img_path.name == mask_p.name:\n",
    "            return mask_p\n",
    "\n",
    "def get_nuclei_dicts_tnbc():\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \"\"\" \n",
    "    dataset_dicts_tnbc_pickle_filepath = Path('./dataset_dicts/dataset_dicts_tnbc.pickle')\n",
    "    if dataset_dicts_tnbc_pickle_filepath.exists():\n",
    "        return load_pickle(dataset_dicts_tnbc_pickle_filepath)\n",
    "    else:\n",
    "        tnbc_image_paths = get_all_file_paths_recursively(TNBC_SLIDES)\n",
    "        tnbc_mask_paths = get_all_file_paths_recursively(TNBC_MASKS)\n",
    "        \n",
    "        dataset_dicts = []       \n",
    "        for img_path in tqdm(tnbc_image_paths):\n",
    "            record = {}\n",
    "            \n",
    "            mask_path = tnbc_find_mask_for_image(img_path, tnbc_mask_paths)\n",
    "            mask_merged = cv2.imread(str(mask_path))[:,:,0]\n",
    "                               \n",
    "            record[\"file_name\"] = img_path\n",
    "            record[\"image_id\"] = img_path.name\n",
    "            record[\"height\"] = mask_merged.shape[0]\n",
    "            record[\"width\"] = mask_merged.shape[1]\n",
    "                   \n",
    "            masks = merged_mask_to_list_of_single_masks(merged_mask=mask_merged, \n",
    "                                                        noise_removal=True, \n",
    "                                                        distance_transform_threshold=0.0)    \n",
    "            \n",
    "            objs = []\n",
    "            for mask in masks:\n",
    "                obj = {\"bbox\": list(mask_to_bbox(mask.astype(np.uint8))),\n",
    "                        \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                        \"segmentation\": [get_polygon_from_binary_mask(mask)],\n",
    "                        #\"segmentation\": rle_encode_pycoco(mask),\n",
    "                        \"category_id\": 0,\n",
    "                        \"iscrowd\": 0}\n",
    "                ### detectron2 lib throws AssertionError if not (len(polygon) % 2 == 0 and len(polygon) >= 6)\n",
    "                if len(obj[\"segmentation\"][0]) >= 6:\n",
    "                    objs.append(obj)\n",
    "                    \n",
    "            # free memory, otherwise kernel will die\n",
    "            masks = None\n",
    "            mask_merged = None\n",
    "            record[\"annotations\"] = objs\n",
    "            dataset_dicts.append(record)\n",
    "            \n",
    "        save_as_pickle(dataset_dicts, dataset_dicts_tnbc_pickle_filepath)    \n",
    "        return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_dicts_tnbc = get_nuclei_dicts_tnbc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DatasetCatalog.clear()\n",
    "DatasetCatalog.register(dataset_name_tnbc, lambda : get_nuclei_dicts_tnbc)\n",
    "MetadataCatalog.get(dataset_name_tnbc).set(thing_classes=[\"nucleus\"])\n",
    "metadata_nuclei_tnbc = MetadataCatalog.get(dataset_name_tnbc)\n",
    "\n",
    "visualize(random.sample(dataset_dicts_tnbc,4), \n",
    "          metadata_nuclei_tnbc,\n",
    "         ground_truth_available=True,\n",
    "         with_preds=False, \n",
    "         predictor=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### PSB_2015_CrowdSourcingNucleiAnnotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "PSB_2015 = PATH/'PSB_2015_CrowdSourcingNucleiAnnotation'/'PSB_2015_ImageSize_400'\n",
    "PSB_2015_IMAGES = PSB_2015/'Original_Images'\n",
    "PSB_2015_MASKS = PSB_2015/'Nuclei_Segmentation'/'AutomatedMethodSegmentation'\n",
    "\n",
    "dataset_name_psb_2015 = 'psb_2015'\n",
    "\n",
    "def psb_2015_img_name_to_id(img_name:str):\n",
    "    splits = img_name.split('_')\n",
    "    return f'{splits[0]}_{splits[1]}'\n",
    "\n",
    "def psb_2015_find_mask_for_image(img_path, mask_paths:List[pathlib.Path]):\n",
    "    for mask_p in mask_paths:\n",
    "        if psb_2015_img_name_to_id(img_path.stem) == psb_2015_img_name_to_id(mask_p.stem):\n",
    "            return mask_p\n",
    "\n",
    "def get_nuclei_dicts_psb_2015():\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \"\"\" \n",
    "    dataset_dicts_psb_2015_pickle_filepath = Path('./dataset_dicts/dataset_dicts_psb_2015.pickle')\n",
    "    if dataset_dicts_psb_2015_pickle_filepath.exists():\n",
    "        return load_pickle(dataset_dicts_psb_2015_pickle_filepath)\n",
    "    else:\n",
    "        psb_2015_image_paths = [p for p in PSB_2015_IMAGES.ls() if p.suffix == '.tiff']\n",
    "        psb_2015_mask_paths = [p for p in PSB_2015_MASKS.ls() if p.suffix == '.tif']\n",
    "        \n",
    "        dataset_dicts = []       \n",
    "        for img_path in tqdm(psb_2015_image_paths):\n",
    "            record = {}\n",
    "            \n",
    "            mask_path = psb_2015_find_mask_for_image(img_path, psb_2015_mask_paths)\n",
    "            mask_merged = cv2.imread(str(mask_path))[:,:,0]\n",
    "                               \n",
    "            record[\"file_name\"] = img_path\n",
    "            record[\"image_id\"] = img_path.name\n",
    "            record[\"height\"] = mask_merged.shape[0]\n",
    "            record[\"width\"] = mask_merged.shape[1]\n",
    "                   \n",
    "            masks = merged_mask_to_list_of_single_masks(merged_mask=mask_merged, \n",
    "                                                        noise_removal=True, \n",
    "                                                        distance_transform_threshold=0.0)    \n",
    "            \n",
    "            objs = []\n",
    "            for mask in masks:\n",
    "                obj = {\"bbox\": list(mask_to_bbox(mask.astype(np.uint8))),\n",
    "                        \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                        \"segmentation\": [get_polygon_from_binary_mask(mask)],\n",
    "                        #\"segmentation\": rle_encode_pycoco(mask),\n",
    "                        \"category_id\": 0,\n",
    "                        \"iscrowd\": 0}\n",
    "                ### detectron2 lib throws AssertionError if not (len(polygon) % 2 == 0 and len(polygon) >= 6)\n",
    "                if len(obj[\"segmentation\"][0]) >= 6:\n",
    "                    objs.append(obj)\n",
    "                    \n",
    "            # free memory, otherwise kernel will die\n",
    "            masks = None\n",
    "            mask_merged = None\n",
    "            record[\"annotations\"] = objs\n",
    "            dataset_dicts.append(record)\n",
    "            \n",
    "        save_as_pickle(dataset_dicts, dataset_dicts_psb_2015_pickle_filepath)    \n",
    "        return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_dicts_psb_2015 = get_nuclei_dicts_psb_2015()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DatasetCatalog.clear()\n",
    "DatasetCatalog.register(dataset_name_psb_2015, lambda : get_nuclei_dicts_psb_2015)\n",
    "MetadataCatalog.get(dataset_name_psb_2015).set(thing_classes=[\"nucleus\"])\n",
    "metadata_nuclei_psb_2015 = MetadataCatalog.get(dataset_name_psb_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visualize(random.sample(dataset_dicts_psb_2015,4), \n",
    "          metadata_nuclei_psb_2015,\n",
    "         ground_truth_available=True,\n",
    "         with_preds=False, \n",
    "         predictor=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### PanNuke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_name_pannuke = 'pannuke'\n",
    "PANNUKE = PATH/'PanNuke_dataset'\n",
    "PANNUKE_IMAGES = PANNUKE/'images'\n",
    "PANNUKE_MASKS = PANNUKE/'masks'\n",
    "####\n",
    "# only needed once, to get pannuke dataset from original shape into more convenient shape\n",
    "####\n",
    "#PANNUKE_IMAGES.mkdir(exist_ok=True)\n",
    "#PANNUKE_MASKS.mkdir(exist_ok=True)\n",
    "#PANNUKE_IMAGES_1 = PANNUKE/'images_fold1.npy'\n",
    "#PANNUKE_MASKS_1 = PANNUKE/'masks_fold1.npy'\n",
    "#PANNUKE_TYPES_1 = PANNUKE/'types_fold1.npy'\n",
    "#PANNUKE_IMAGES_2 = PANNUKE/'images_fold2.npy'\n",
    "#PANNUKE_MASKS_2 = PANNUKE/'masks_fold2.npy'\n",
    "#PANNUKE_TYPES_2 = PANNUKE/'types_fold2.npy'\n",
    "#PANNUKE_IMAGES_3 = PANNUKE/'images_fold3.npy'\n",
    "#PANNUKE_MASKS_3 = PANNUKE/'masks_fold3.npy'\n",
    "#PANNUKE_TYPES_3 = PANNUKE/'types_fold3.npy'\n",
    "#\n",
    "#pannuke_imgs_all = np.concatenate([np.load(PANNUKE_IMAGES_1).astype(np.uint8), \n",
    "#                                   np.load(PANNUKE_IMAGES_2).astype(np.uint8), \n",
    "#                                   np.load(PANNUKE_IMAGES_3).astype(np.uint8)])\n",
    "#\n",
    "#pannuke_masks_all = np.concatenate([np.load(PANNUKE_MASKS_1).astype(np.uint8), \n",
    "#                                    np.load(PANNUKE_MASKS_2).astype(np.uint8), \n",
    "#                                    np.load(PANNUKE_MASKS_3).astype(np.uint8)])\n",
    "#\n",
    "#for n, img in enumerate(pannuke_imgs_all[:1]):   \n",
    "#    f, axarr = plt.subplots(1,7, figsize=(20,20))\n",
    "#    axarr[0].imshow(img)                       \n",
    "#    for i in range(6):\n",
    "#        axarr[i+1].imshow(pannuke_masks_all[n,:,:,i])        \n",
    "#    plt.show()\n",
    "#\n",
    "#### save all images from array to disk as png \n",
    "#for n, img_array in enumerate(tqdm(pannuke_imgs_all)):\n",
    "#    img_pil = PIL.Image.fromarray(img_array)\n",
    "#    img_pil.save(PANNUKE_IMAGES/f\"pannuke_img_{n:04d}.png\",\"PNG\")\n",
    "#\n",
    "#### extract masks for every single instance from the different merged masks and save them to disk\n",
    "#for n, mask_array in enumerate(tqdm(pannuke_masks_all)):\n",
    "#    masks = [mask_array[:,:,i] for i in range(5)]\n",
    "#    single_masks = []\n",
    "#    for m in masks[0:5]:\n",
    "#        if np.mean(m) != 0.0:\n",
    "#            for i in range(1,np.max(m)+1):\n",
    "#                 if i in m:\n",
    "#                    single_masks.append((m==i).astype(np.uint8)*255)\n",
    "#    \n",
    "#    directory = PANNUKE_MASKS/f\"{n:04d}\"\n",
    "#    directory.mkdir(exist_ok=True)\n",
    "#    for m, single_mask in enumerate(single_masks):\n",
    "#        #print(np.max(single_mask))\n",
    "#        #imshow(single_mask)\n",
    "#        #plt.show()\n",
    "#        mask_pil = PIL.Image.fromarray(single_mask)\n",
    "#        mask_pil.save(directory/f\"pannuke_mask_{n:04d}_{m}.png\",\"PNG\")\n",
    "\n",
    "def pannuke_img_name_to_id(img_name:str):\n",
    "    splits = img_name.split('_')\n",
    "    return f'{splits[2]}'\n",
    "\n",
    "def pannuke_find_mask_folder_for_image(img_path, mask_folder_paths:List[pathlib.Path]):\n",
    "    for mask_p in mask_folder_paths:\n",
    "        if pannuke_img_name_to_id(img_path.stem) == mask_p.stem:\n",
    "            return mask_p\n",
    "\n",
    "def get_nuclei_dicts_pannuke():\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    \"\"\" \n",
    "    dataset_dicts_pannuke_pickle_filepath = Path('./dataset_dicts/dataset_dicts_pannuke.pickle')\n",
    "    if dataset_dicts_pannuke_pickle_filepath.exists():\n",
    "        return load_pickle(dataset_dicts_pannuke_pickle_filepath)\n",
    "    else:        \n",
    "        dataset_dicts = []       \n",
    "        for img_path in tqdm([p for p in PANNUKE_IMAGES.ls() if p.suffix == '.png']):\n",
    "            record = {}\n",
    "            \n",
    "            img_pil = PIL.Image.open(img_path)\n",
    "                                           \n",
    "            record[\"file_name\"] = img_path\n",
    "            record[\"image_id\"] = pannuke_img_name_to_id(img_path.stem)\n",
    "            record[\"height\"] = img_pil.height\n",
    "            record[\"width\"] = img_pil.width\n",
    "                   \n",
    "            masks = [open_mask_as_np(mp) for mp in \\\n",
    "                     pannuke_find_mask_folder_for_image(img_path, PANNUKE_MASKS.ls()).ls()]\n",
    "            \n",
    "            objs = []\n",
    "            for mask in masks:\n",
    "                obj = {\"bbox\": list(mask_to_bbox(mask.astype(np.uint8))),\n",
    "                        \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                        \"segmentation\": [get_polygon_from_binary_mask(mask)],\n",
    "                        #\"segmentation\": rle_encode_pycoco(mask),\n",
    "                        \"category_id\": 0,\n",
    "                        \"iscrowd\": 0}\n",
    "                ### detectron2 lib throws AssertionError if not (len(polygon) % 2 == 0 and len(polygon) >= 6)\n",
    "                if len(obj[\"segmentation\"][0]) >= 6:\n",
    "                    objs.append(obj)\n",
    "                    \n",
    "            # free memory, otherwise kernel will die\n",
    "            masks = None\n",
    "            record[\"annotations\"] = objs\n",
    "            dataset_dicts.append(record)\n",
    "            \n",
    "        save_as_pickle(dataset_dicts, dataset_dicts_pannuke_pickle_filepath)    \n",
    "        return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_dicts_pannuke = get_nuclei_dicts_pannuke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DatasetCatalog.clear()\n",
    "DatasetCatalog.register(dataset_name_pannuke, lambda : get_nuclei_dicts_pannuke)\n",
    "MetadataCatalog.get(dataset_name_pannuke).set(thing_classes=[\"nucleus\"])\n",
    "metadata_nuclei_pannuke = MetadataCatalog.get(dataset_name_pannuke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visualize(random.sample(dataset_dicts_pannuke,4), \n",
    "          metadata_nuclei_pannuke,\n",
    "         ground_truth_available=True,\n",
    "         with_preds=False, \n",
    "         predictor=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## combine all dataset dicts that have ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_dicts_dsb18_train_and_test1 = get_nuclei_dicts_dsb18(isTestSet=False)\n",
    "dataset_dicts_monuseg18 = get_nuclei_dicts_monuseg18()\n",
    "dataset_dicts_consep = get_nuclei_dicts_consep()\n",
    "dataset_dicts_janowczyk = get_nuclei_dicts_janowczyk()\n",
    "dataset_dicts_tnbc = get_nuclei_dicts_tnbc()\n",
    "dataset_dicts_psb_2015 = get_nuclei_dicts_psb_2015()\n",
    "dataset_dicts_pannuke = get_nuclei_dicts_pannuke()\n",
    "\n",
    "dataset_dicts_all_combined = dataset_dicts_dsb18_train_and_test1 + \\\n",
    "                                dataset_dicts_monuseg18 + \\\n",
    "                                dataset_dicts_consep + \\\n",
    "                                dataset_dicts_janowczyk + \\\n",
    "                                dataset_dicts_tnbc + \\\n",
    "                                dataset_dicts_psb_2015 + \\\n",
    "                                dataset_dicts_pannuke\n",
    "\n",
    "print(len(dataset_dicts_all_combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## some visualization of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize(dataset_dicts=random.sample(dataset_dicts_all_combined, 3), \n",
    "          metadata=metadata_nuclei_all_combined, \n",
    "          ground_truth_available=True, \n",
    "          with_preds=False, predictor=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## sort dataset by number of nuclei, inspect it and exclude some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#key: img_path; value: number of nuclei\n",
    "path_to_n_objects = {}\n",
    "\n",
    "for ds in [dataset_dicts_all_combined]:\n",
    "    for d in ds:\n",
    "        path_to_n_objects[d['file_name']] = len(d['annotations'])\n",
    "        \n",
    "#sort by number of nuclei\n",
    "path_to_n_objects_ascending = {k: v for k, v in sorted(path_to_n_objects.items(), key=lambda item: item[1], reverse=False)}\n",
    "path_to_n_objects_descending = {k: v for k, v in sorted(path_to_n_objects.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'max number of objects: {list(path_to_n_objects_descending.values())[0]}')\n",
    "print(f'min number of objects: {list(path_to_n_objects_descending.values())[-1]}')\n",
    "print(f'mean number of objects: {sum(list(path_to_n_objects.values()))/len(list(path_to_n_objects.values()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k, v in list(path_to_n_objects_ascending.items())[100:200]:\n",
    "    print(k)\n",
    "    print(v)\n",
    "    show_pil(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "psb_2015_mask_paths = [p for p in PSB_2015_MASKS.ls() if p.suffix == '.tif']\n",
    "\n",
    "for k, v in list(path_to_n_objects_ascending.items())[:4]:\n",
    "    print(k)\n",
    "    print(v)\n",
    "    mask_path = psb_2015_find_mask_for_image(k, psb_2015_mask_paths)\n",
    "    mask = open_mask_as_np(mask_path)\n",
    " \n",
    "    f, axarr = plt.subplots(1,2, figsize=(5,5))\n",
    "    axarr[0].imshow(PIL.Image.open(k))\n",
    "    axarr[1].imshow(mask) \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "excluded_paths = \\\n",
    "['/home/Deep_Learner/private/datasets/nuclei_datasets/PanNuke_dataset/images/pannuke_img_2132.png',\n",
    "'/home/Deep_Learner/private/datasets/nuclei_datasets/PanNuke_dataset/images/pannuke_img_4589.png',\n",
    "'/home/Deep_Learner/private/datasets/nuclei_datasets/PanNuke_dataset/images/pannuke_img_4588.png',\n",
    "'/home/Deep_Learner/private/datasets/nuclei_datasets/PanNuke_dataset/images/pannuke_img_2134.png',\n",
    "\n",
    "'/home/Deep_Learner/private/datasets/nuclei_datasets/PSB_2015_CrowdSourcingNucleiAnnotation/PSB_2015_ImageSize_400/Original_Images/9300_11800.tiff',\n",
    "'/home/Deep_Learner/private/datasets/nuclei_datasets/PSB_2015_CrowdSourcingNucleiAnnotation/PSB_2015_ImageSize_400/Original_Images/11300_11000.tiff',\n",
    "'/home/Deep_Learner/private/datasets/nuclei_datasets/PSB_2015_CrowdSourcingNucleiAnnotation/PSB_2015_ImageSize_400/Original_Images/11700_12200.tiff',\n",
    "'/home/Deep_Learner/private/datasets/nuclei_datasets/PSB_2015_CrowdSourcingNucleiAnnotation/PSB_2015_ImageSize_400/Original_Images/8500_11000.tiff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_dicts_all_combined_without_excluded = [d for d in dataset_dicts_all_combined if str(d['file_name']) \\\n",
    "                                               not in excluded_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(len(dataset_dicts_all_combined))\n",
    "print(len(dataset_dicts_all_combined_without_excluded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Split into train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataset_dicts_train, dataset_dicts_valid =  train_test_split(dataset_dicts_all_combined_without_excluded, \\\n",
    "                                                             test_size=0.05, random_state=seed)\n",
    "\n",
    "print(len(dataset_dicts_all_combined_without_excluded))\n",
    "print(len(dataset_dicts_train))\n",
    "print(len(dataset_dicts_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## register datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DatasetCatalog.clear()\n",
    "\n",
    "DatasetCatalog.register(dataset_name_all_combined_without_excluded, lambda : dataset_dicts_all_combined_without_excluded)\n",
    "MetadataCatalog.get(dataset_name_all_combined_without_excluded).set(thing_classes=[\"nucleus\"])\n",
    "\n",
    "DatasetCatalog.register(dataset_name_train, lambda : dataset_dicts_train)\n",
    "MetadataCatalog.get(dataset_name_train).set(thing_classes=[\"nucleus\"])\n",
    "\n",
    "DatasetCatalog.register(dataset_name_valid, lambda : dataset_dicts_valid)\n",
    "MetadataCatalog.get(dataset_name_valid).set(thing_classes=[\"nucleus\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metadata_nuclei_all_combined_without_excluded = MetadataCatalog.get(dataset_name_all_combined_without_excluded)\n",
    "metadata_nuclei_train = MetadataCatalog.get(dataset_name_train)\n",
    "metadata_nuclei_valid = MetadataCatalog.get(dataset_name_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output_directory = Path('./output/3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "epochs = 40\n",
    "bs = 6\n",
    "iterations = int(len(dataset_dicts_train)/bs*epochs);iterations\n",
    "\n",
    "\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "\n",
    "cfg.DATASETS.TRAIN = (dataset_name_train,)\n",
    "cfg.DATASETS.TEST = ()\n",
    "\n",
    "cfg.DATALOADER.NUM_WORKERS = 20\n",
    "\n",
    "cfg.SOLVER.IMS_PER_BATCH = bs\n",
    "cfg.SOLVER.BASE_LR = 2.5e-3  \n",
    "cfg.SOLVER.MAX_ITER = iterations\n",
    "cfg.SOLVER.LR_SCHEDULER_NAME = 'WarmupCosineLR'\n",
    "cfg.SOLVER.WARMUP_ITERS = 0.3*iterations\n",
    "#cfg.SOLVER.LR_SCHEDULER_NAME = 'OneCycleLR'\n",
    "\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 6  # (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (nucleus)\n",
    "cfg.MODEL.RPN.PRE_NMS_TOPK_TRAIN = 12000\n",
    "cfg.MODEL.RPN.POST_NMS_TOPK_TRAIN = 2000\n",
    "\n",
    "cfg['SEED'] = seed\n",
    "\n",
    "cfg.OUTPUT_DIR = str(output_directory)\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#save_as_pickle(cfg, Path(cfg.OUTPUT_DIR)/'cfg.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = DefaultTrainer(cfg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#trainer.resume_or_load(resume=False)\n",
    "trainer.resume_or_load(resume=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference & evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = Path('./output/3') \n",
    "weights_path = Path(output_directory/'model_final.pth')\n",
    "\n",
    "cfg = load_pickle(output_directory/'cfg.pickle')\n",
    "cfg['SEED'] = seed\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 20\n",
    "cfg.MODEL.WEIGHTS = str(weights_path)\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\n",
    "cfg.MODEL.RPN.PRE_NMS_TOPK_TEST = 10000\n",
    "cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 3000\n",
    "cfg.TEST.DETECTIONS_PER_IMAGE = cfg.MODEL.RPN.POST_NMS_TOPK_TEST\n",
    "\n",
    "#cfg.MODEL.DEVICE = 'cpu'\n",
    "#cfg.MODEL.DEVICE = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DatasetCatalog.clear()\n",
    "cfg.DATASETS.TEST = (dataset_name_valid, )\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### sort dataset dict by different metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### by relative error in predicted number of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## add number of predicted instances and the relative error to every entry in dataset_dicts_nuclei_valid\n",
    "\n",
    "for d in tqdm(dataset_dicts_valid[:]):\n",
    "    n_pred = len(predictor(cv2.imread(str(d[\"file_name\"])))['instances'])\n",
    "    d['n_predicted'] = n_pred\n",
    "    n_act = len(d['annotations'])+0.0001 #to prevent division by zero\n",
    "    d['relative_error'] = abs(n_pred-n_act)/n_act    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## sort it descending by relative_error\n",
    "dataset_dicts_valid.sort(key=lambda d: d['relative_error'], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### by IoU with ground truth  // TODO: function from polygon representation to binary mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# slight modification of this implementation\n",
    "#https://www.kaggle.com/iezepov/fast-iou-scoring-metric-in-pytorch-and-numpy\n",
    "SMOOTH = 1e-6\n",
    "def iou_pytorch(predicted_mask: torch.Tensor, ground_truth_mask: torch.Tensor):    \n",
    "    intersection = (predicted_mask & ground_truth_mask).float().sum((0, 1))  # Will be zero if Truth=0 or Prediction=0\n",
    "    union = (predicted_mask | ground_truth_mask).float().sum((0, 1))         # Will be zero if both are 0\n",
    "    \n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n",
    "    \n",
    "    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n",
    "    \n",
    "    return thresholded  # Or thresholded.mean() if you are interested in average across the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for d in tqdm(dataset_dicts_valid[:]):\n",
    "    pred = predictor(cv2.imread(str(d[\"file_name\"])))\n",
    "    ### merged mask of prediction\n",
    "    mask_preds = merge_masks(pred['instances'].get('pred_masks').cpu(), shape=(d['height'], d['width']))\n",
    "    ### merged mask of ground truth\n",
    "    path = d[\"file_name\"]\n",
    "    masks = None\n",
    "    try:\n",
    "        masks = [open_mask_as_np(mp) for mp in get_masks_path_from_id(path.stem, base_path=path.parent.parent.parent).ls()]    \n",
    "    except FileNotFoundError:\n",
    "        masks = get_mask_list_from_rle_for_one_id(path.stem, df_rle_labels, shape=(d['width'],d['height']))\n",
    "\n",
    "    mask_actual = merge_masks(masks, shape=(d['height'], d['width']))\n",
    "    d['iou'] = iou_pytorch(mask_preds, mask_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## sort it ascending by iou \n",
    "dataset_dicts_valid.sort(key=lambda d: d['iou'], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### visualize most wrongly predicted cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualize(dataset_dicts=dataset_dicts_valid[0:5], \n",
    "          metadata=metadata_nuclei_valid,\n",
    "          ground_truth_available=True,\n",
    "          with_preds=True,\n",
    "          predictor=predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## dsb 2018 stage2 test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### register dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "DatasetCatalog.clear()\n",
    "\n",
    "DatasetCatalog.register(dataset_name_dsb18_test2, lambda : get_nuclei_dicts_dsb18(True))\n",
    "MetadataCatalog.get(dataset_name_dsb18_test2).set(thing_classes=[\"nucleus\"])\n",
    "cfg.DATASETS.TEST = (dataset_name_dsb18_test2, )\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "metadata_nuclei_dsb18_test2 = MetadataCatalog.get(dataset_name_dsb18_test2)\n",
    "dataset_dicts_dsb18_test2 = get_nuclei_dicts_dsb18(isTestSet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "visualize(dataset_dicts=dataset_dicts_dsb18_test2[:2], \n",
    "          metadata=metadata_nuclei_dsb18_test2,\n",
    "          ground_truth_available=False,\n",
    "          with_preds=True,\n",
    "          predictor=predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = {}\n",
    "for d['file_name'] in tqdm(dataset_dicts_dsb18_test2):\n",
    "    p = d['file_name']\n",
    "    img = cv2.imread(str(p))\n",
    "    with torch.no_grad():\n",
    "        pred = predictor(img)\n",
    "    preds[p.stem] = (pred[\"instances\"].to(\"cpu\"))\n",
    "    del pred\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### create submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame(columns=['ImageId', 'EncodedPixels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for i, pred in tqdm(list(preds.items())[:]):\n",
    "    masks = pred.get('pred_masks')\n",
    "    if len(masks) == 0:\n",
    "        df_sub = df_sub.append({'ImageId': i, 'EncodedPixels': ''}, ignore_index=True)\n",
    "    else:\n",
    "        ###\n",
    "        # the competition evaluation at kaggle throws an error, if an encoded pixel belongs to more than one object\n",
    "        ###\n",
    "        mask_merged = None\n",
    "        for mask in masks:\n",
    "            mask = mask.int()\n",
    "            if mask_merged == None:\n",
    "                mask_merged = mask\n",
    "            else:\n",
    "                overlap = (mask == mask_merged).int()\n",
    "                mask = mask - overlap\n",
    "                mask_merged = merge_masks([mask_merged, mask], shape=(mask.shape[0],mask.shape[1]))\n",
    "            df_sub = df_sub.append({'ImageId': i, 'EncodedPixels': rle_from_list_to_string(rle_encode(mask))}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "submission_path = Path('./submission_detectron2_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_sub.to_csv(submission_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.read_csv(submission_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Muscles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MUSCLES = Path('/home/Deep_Learner/private/datasets/Muskeln')\n",
    "MUSCLES_WSIS = MUSCLES/'wsis'\n",
    "MUSCLES_ROIS = MUSCLES/'rois'\n",
    "MUSCLES_TILES = MUSCLES/'tiles'\n",
    "dataset_name_muscle = 'nuclei_muscle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def func(x):\n",
    "    return 1.0 - (10.0 / (10.0 + x))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.array(range(100))\n",
    "\n",
    "y = func(x)\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('x axis')\n",
    "plt.ylabel('y axis')\n",
    "plt.grid(alpha=.4,linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naming_func_muscles(wsi_path:pathlib.Path):\n",
    "    path = Path(wsi_path)\n",
    "    split = path.stem.split('-')\n",
    "    return f'{split[0]}'[:-1]\n",
    "\n",
    "def scoring_function_muscle(tissue_percent, combined_factor):\n",
    "    \"\"\"\n",
    "    This favors pink over purple regions (muscle tissue has few nuclei and lots of sarcoplam, which\n",
    "    is pink in H&E staining). color_factor and s_and_v_factor are higher, \n",
    "    the more hematoxylin stained tissue is in the image\n",
    "    \"\"\"\n",
    "    if(combined_factor == 0):\n",
    "        return tissue_percent / 2\n",
    "    else:\n",
    "        return (100/(combined_factor)) * tissue_percent / 100       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c70f557d73470c9c5e53d1dcc732db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process /home/Deep_Learner/private/datasets/Muskeln/wsis/DKO474_US_hom_1_HE - 2020-04-07 11.35.13.ndpi\n",
      "Starting to process /home/Deep_Learner/private/datasets/Muskeln/wsis/DKO474_US_hom_3_HE - 2020-04-07 11.46.24.ndpi\n",
      "Starting to process /home/Deep_Learner/private/datasets/Muskeln/wsis/DKO474_US_hom_2_HE - 2020-04-07 11.40.20.ndpi\n",
      "Starting to process /home/Deep_Learner/private/datasets/Muskeln/wsis/DKO476_US_wt_2_HE - 2020-04-07 12.08.00.ndpi\n",
      "Starting to process /home/Deep_Learner/private/datasets/Muskeln/wsis/DKO476_US_wt_5_HE - 2020-04-07 12.13.27.ndpi\n",
      "Starting to process /home/Deep_Learner/private/datasets/Muskeln/wsis/DKO476_US_wt_3_HE - 2020-04-07 12.09.33.ndpi\n",
      "Starting to process /home/Deep_Learner/private/datasets/Muskeln/wsis/DKO476_US_wt_7_HE - 2020-04-07 12.21.42.ndpi\n",
      "Starting to process /home/Deep_Learner/private/datasets/Muskeln/wsis/DKO476_US_wt_4_HE - 2020-04-07 12.11.11.ndpi\n",
      "Starting to process /home/Deep_Learner/private/datasets/Muskeln/wsis/DKO482_US_hom_5_HE - 2020-04-07 11.27.52.ndpi\n",
      "Starting to process /home/Deep_Learner/private/datasets/Muskeln/wsis/DKO482_US_hom_4_HE - 2020-04-07 11.25.21.ndpi\n",
      "Starting to process /home/Deep_Learner/private/datasets/Muskeln/wsis/DKO482_US_hom_7_HE - 2020-04-07 11.29.54.ndpi\n",
      "Starting to process /home/Deep_Learner/private/datasets/Muskeln/wsis/DKO482_US_hom_8_HE - 2020-04-07 11.31.56.ndpi\n",
      "Starting to process /home/Deep_Learner/private/datasets/Muskeln/wsis/DKO482_US_hom_9_HE - 2020-04-07 11.33.28.ndpi\n",
      "Starting to process /home/Deep_Learner/private/datasets/Muskeln/wsis/DKO488_US_wt_10_HE - 2020-04-07 12.04.51.ndpi\n",
      "Starting to process /home/Deep_Learner/private/datasets/Muskeln/wsis/DKO488_US_wt_12_HE - 2020-04-07 12.06.24.ndpi\n",
      "Starting to process /home/Deep_Learner/private/datasets/Muskeln/wsis/DKO488_US_wt_7_HE - 2020-04-07 11.59.21.ndpi\n",
      "Starting to process /home/Deep_Learner/private/datasets/Muskeln/wsis/DKO488_US_wt_9_HE - 2020-04-07 12.03.11.ndpi\n",
      "Starting to process /home/Deep_Learner/private/datasets/Muskeln/wsis/DKO488_US_wt_8_HE - 2020-04-07 12.01.37.ndpi\n",
      "Starting to process /home/Deep_Learner/private/datasets/Muskeln/wsis/DKO474_US_hom_5_HE - 2020-04-07 11.56.06.ndpi\n",
      "Starting to process /home/Deep_Learner/private/datasets/Muskeln/wsis/DKO474_US_hom_4_HE - 2020-04-07 11.50.40.ndpi\n"
     ]
    }
   ],
   "source": [
    "### against DecompressionBombWarning\n",
    "from PIL import Image\n",
    "Image.MAX_IMAGE_PIXELS = 115990855680000 \n",
    "\n",
    "tile_score_thresh=0.5\n",
    "tilesummaries_muscles_path = Path(f'./tilesummaries_muscles_{tile_score_thresh}.pickle')\n",
    "\n",
    "\n",
    "if os.path.isfile(tilesummaries_muscles_path):\n",
    "    ###\n",
    "    # just load from disc, if you have already calculated tile infos before\n",
    "    ###\n",
    "    tilesummaries_muscles = load_pickle(tilesummaries_muscles_path)\n",
    "else:\n",
    "    ###\n",
    "    # generate and save tile info\n",
    "    ###\n",
    "    tilesummaries_muscles = tiles.WsiOrROIToTilesMultithreaded(wsiPaths = MUSCLES_WSIS.ls(), \n",
    "                                                       tilesFolderPath=None, \n",
    "                                                       tileWidth=512, \n",
    "                                                       tileHeight=512, \n",
    "                                                       tile_naming_func = naming_func_muscles, \n",
    "                                                       save_tiles=False,\n",
    "                                                       tileScoringFunction = scoring_function_muscle, \n",
    "                                                       is_wsi=True, \n",
    "                                                       tile_score_thresh=tile_score_thresh, \n",
    "                                                       return_as_tilesummary_object=True)\n",
    "    #save_as_pickle(tilesummaries_muscles, tilesummaries_muscles_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ts in tilesummaries_muscles:\n",
    "    tiles.show_wsi_with_marked_tiles(tilesummary=ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset getter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_image_from_wsi(dataset_dict,\n",
    "                        convert_mode:str='RGB')->PIL.Image:\n",
    "    tile = dataset_dict['tile']\n",
    "    wsi_path = tile.wsi_path\n",
    "    x = tile.get_x()\n",
    "    y = tile.get_y()\n",
    "    width = tile.get_width()\n",
    "    height = tile.get_height()\n",
    "    level = tile.level\n",
    "    tile = tiles.ExtractTileFromWSI(path=wsi_path, x=x, y=y, width=width, height=height, level=level)\n",
    "    tile = tile.convert(convert_mode)\n",
    "    #tile = fastai.vision.image.pil2tensor(tile,np.float32)\n",
    "    return tile\n",
    "\n",
    "from fvcore.common.file_io import PathManager\n",
    "#detectron2.data.detection_utils.read_image\n",
    "def read_image_custom(dataset_dict, format=None):\n",
    "    \"\"\"\n",
    "    Read an image into the given format.\n",
    "    Will apply rotation and flipping if the image has such exif information.\n",
    "    Args:\n",
    "        dataset_dict: \n",
    "        format (str): one of the supported image modes in PIL, or \"BGR\"\n",
    "    Returns:\n",
    "        image (np.ndarray): an HWC image in the given format.\n",
    "    \"\"\"\n",
    "    image = open_image_from_wsi(dataset_dict=dataset_dict)\n",
    "\n",
    "    # capture and ignore this bug: https://github.com/python-pillow/Pillow/issues/3973\n",
    "    try:\n",
    "        image = ImageOps.exif_transpose(image)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if format is not None:\n",
    "        # PIL only supports RGB, so convert to RGB and flip channels over below\n",
    "        conversion_format = format\n",
    "        if format == \"BGR\":\n",
    "            conversion_format = \"RGB\"\n",
    "        image = image.convert(conversion_format)\n",
    "    image = np.asarray(image)\n",
    "    if format == \"BGR\":\n",
    "        # flip channels if needed\n",
    "        image = image[:, :, ::-1]\n",
    "    # PIL squeezes out the channel dimension for \"L\", so make it HWC\n",
    "    if format == \"L\":\n",
    "        image = np.expand_dims(image, -1)\n",
    "    return image\n",
    "\n",
    "detectron2.data.detection_utils.read_image = read_image_custom\n",
    "\n",
    "\n",
    "\n",
    "#https://detectron2.readthedocs.io/tutorials/data_loading.html\n",
    "#https://detectron2.readthedocs.io/_modules/detectron2/data/dataset_mapper.html#DatasetMapper\n",
    "import copy\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "from fvcore.common.file_io import PathManager\n",
    "from PIL import Image\n",
    "\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.data import transforms as T\n",
    "\n",
    "\"\"\"\n",
    "This file contains the default mapping that's applied to \"dataset dicts\".\n",
    "\"\"\"\n",
    "\n",
    "__all__ = [\"DatasetMapper\"]\n",
    "\n",
    "### changes to default:\n",
    "# - in __call__ : read_image_custom \n",
    "class DatasetMapper_Custom:\n",
    "    \"\"\"\n",
    "    A callable which takes a dataset dict in Detectron2 Dataset format,\n",
    "    and map it into a format used by the model.\n",
    "\n",
    "    This is the default callable to be used to map your dataset dict into training data.\n",
    "    You may need to follow it to implement your own one for customized logic,\n",
    "    such as a different way to read or transform images.\n",
    "    See :doc:`/tutorials/data_loading` for details.\n",
    "\n",
    "    The callable currently does the following:\n",
    "\n",
    "    1. Read the image from \"file_name\"\n",
    "    2. Applies cropping/geometric transforms to the image and annotations\n",
    "    3. Prepare data and annotations to Tensor and :class:`Instances`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, is_train=True):\n",
    "        if cfg.INPUT.CROP.ENABLED and is_train:\n",
    "            self.crop_gen = T.RandomCrop(cfg.INPUT.CROP.TYPE, cfg.INPUT.CROP.SIZE)\n",
    "            logging.getLogger(__name__).info(\"CropGen used in training: \" + str(self.crop_gen))\n",
    "        else:\n",
    "            self.crop_gen = None\n",
    "\n",
    "        self.tfm_gens = utils.build_transform_gen(cfg, is_train)\n",
    "\n",
    "        # fmt: off\n",
    "        self.img_format     = cfg.INPUT.FORMAT\n",
    "        self.mask_on        = cfg.MODEL.MASK_ON\n",
    "        self.mask_format    = cfg.INPUT.MASK_FORMAT\n",
    "        self.keypoint_on    = cfg.MODEL.KEYPOINT_ON\n",
    "        self.load_proposals = cfg.MODEL.LOAD_PROPOSALS\n",
    "        # fmt: on\n",
    "        if self.keypoint_on and is_train:\n",
    "            # Flip only makes sense in training\n",
    "            self.keypoint_hflip_indices = utils.create_keypoint_hflip_indices(cfg.DATASETS.TRAIN)\n",
    "        else:\n",
    "            self.keypoint_hflip_indices = None\n",
    "\n",
    "        if self.load_proposals:\n",
    "            self.min_box_side_len = cfg.MODEL.PROPOSAL_GENERATOR.MIN_SIZE\n",
    "            self.proposal_topk = (\n",
    "                cfg.DATASETS.PRECOMPUTED_PROPOSAL_TOPK_TRAIN\n",
    "                if is_train\n",
    "                else cfg.DATASETS.PRECOMPUTED_PROPOSAL_TOPK_TEST\n",
    "            )\n",
    "        self.is_train = is_train\n",
    "\n",
    "\n",
    "    def __call__(self, dataset_dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_dict (dict): Metadata of one image, in Detectron2 Dataset format.\n",
    "\n",
    "        Returns:\n",
    "            dict: a format that builtin models in detectron2 accept\n",
    "        \"\"\"\n",
    "        dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "        # USER: Write your own image loading if it's not from a file\n",
    "                \n",
    "        #image = utils.read_image(dataset_dict[\"file_name\"], format=self.img_format)\n",
    "        image = read_image_custom(dataset_dict, format=self.img_format)\n",
    "        \n",
    "        \n",
    "        utils.check_image_size(dataset_dict, image)\n",
    "\n",
    "        if \"annotations\" not in dataset_dict:\n",
    "            image, transforms = T.apply_transform_gens(\n",
    "                ([self.crop_gen] if self.crop_gen else []) + self.tfm_gens, image\n",
    "            )\n",
    "        else:\n",
    "            # Crop around an instance if there are instances in the image.\n",
    "            # USER: Remove if you don't use cropping\n",
    "            if self.crop_gen:\n",
    "                crop_tfm = utils.gen_crop_transform_with_instance(\n",
    "                    self.crop_gen.get_crop_size(image.shape[:2]),\n",
    "                    image.shape[:2],\n",
    "                    np.random.choice(dataset_dict[\"annotations\"]),\n",
    "                )\n",
    "                image = crop_tfm.apply_image(image)\n",
    "            image, transforms = T.apply_transform_gens(self.tfm_gens, image)\n",
    "            if self.crop_gen:\n",
    "                transforms = crop_tfm + transforms\n",
    "\n",
    "        image_shape = image.shape[:2]  # h, w\n",
    "\n",
    "        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,\n",
    "        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.\n",
    "        # Therefore it's important to use torch.Tensor.\n",
    "        dataset_dict[\"image\"] = torch.as_tensor(np.ascontiguousarray(image.transpose(2, 0, 1)))\n",
    "\n",
    "        # USER: Remove if you don't use pre-computed proposals.\n",
    "        if self.load_proposals:\n",
    "            utils.transform_proposals(\n",
    "                dataset_dict, image_shape, transforms, self.min_box_side_len, self.proposal_topk\n",
    "            )\n",
    "\n",
    "        if not self.is_train:\n",
    "            # USER: Modify this if you want to keep them for some reason.\n",
    "            dataset_dict.pop(\"annotations\", None)\n",
    "            dataset_dict.pop(\"sem_seg_file_name\", None)\n",
    "            return dataset_dict\n",
    "\n",
    "        if \"annotations\" in dataset_dict:\n",
    "            # USER: Modify this if you want to keep them for some reason.\n",
    "            for anno in dataset_dict[\"annotations\"]:\n",
    "                if not self.mask_on:\n",
    "                    anno.pop(\"segmentation\", None)\n",
    "                if not self.keypoint_on:\n",
    "                    anno.pop(\"keypoints\", None)\n",
    "\n",
    "            # USER: Implement additional transformations if you have other types of data\n",
    "            annos = [\n",
    "                utils.transform_instance_annotations(\n",
    "                    obj, transforms, image_shape, keypoint_hflip_indices=self.keypoint_hflip_indices\n",
    "                )\n",
    "                for obj in dataset_dict.pop(\"annotations\")\n",
    "                if obj.get(\"iscrowd\", 0) == 0\n",
    "            ]\n",
    "            instances = utils.annotations_to_instances(\n",
    "                annos, image_shape, mask_format=self.mask_format\n",
    "            )\n",
    "            # Create a tight bounding box from masks, useful when image is cropped\n",
    "            if self.crop_gen and instances.has(\"gt_masks\"):\n",
    "                instances.gt_boxes = instances.gt_masks.get_bounding_boxes()\n",
    "            dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n",
    "\n",
    "        # USER: Remove if you don't do semantic/panoptic segmentation.\n",
    "        if \"sem_seg_file_name\" in dataset_dict:\n",
    "            with PathManager.open(dataset_dict.pop(\"sem_seg_file_name\"), \"rb\") as f:\n",
    "                sem_seg_gt = Image.open(f)\n",
    "                sem_seg_gt = np.asarray(sem_seg_gt, dtype=\"uint8\")\n",
    "            sem_seg_gt = transforms.apply_segmentation(sem_seg_gt)\n",
    "            sem_seg_gt = torch.as_tensor(sem_seg_gt.astype(\"long\"))\n",
    "            dataset_dict[\"sem_seg\"] = sem_seg_gt\n",
    "        return dataset_dict\n",
    "\n",
    "    \n",
    "detectron2.data.dataset_mapper = DatasetMapper_Custom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nuclei_dicts_muscles(wsi_paths:List[pathlib.Path], pickle_filepath:pathlib.Path):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        \n",
    "    \"\"\"\n",
    "    if pickle_filepath != None and pickle_filepath.exists():\n",
    "        return load_pickle(pickle_filepath)\n",
    "    else:\n",
    "        dataset_dicts = []\n",
    "        n = 0\n",
    "        for ts in tqdm(tilesummaries_muscles):\n",
    "            if ts.wsi_path in wsi_paths:\n",
    "                n += 1\n",
    "                for tile in ts.top_tiles():\n",
    "                    record = {}\n",
    "                    \n",
    "                    record['tile'] = tile\n",
    "                    record[\"file_name\"] = tile.get_name()\n",
    "                    record[\"image_id\"] = tile.get_name()\n",
    "                    record[\"height\"] = tile.get_height()\n",
    "                    record[\"width\"] = tile.get_width()\n",
    "                    \n",
    "                    #no ground truth masks available               \n",
    "                    record[\"annotations\"] = []\n",
    "                    dataset_dicts.append(record)\n",
    "            \n",
    "        #save_as_pickle(dataset_dicts, pickle_filepath)\n",
    "        print(f'Number of WSIs: {n}')\n",
    "        return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412a280148cf4651896efeb14a5c7c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/Deep_Learner/private/datasets/Muskeln/wsis/DKO476_US_wt_7_HE - 2020-04-07 12.21.42.ndpi: Number of tiles that will be kept/all possible tiles: 232/1120\n",
      "/home/Deep_Learner/private/datasets/Muskeln/wsis/DKO488_US_wt_12_HE - 2020-04-07 12.06.24.ndpi: Number of tiles that will be kept/all possible tiles: 198/1120\n",
      "/home/Deep_Learner/private/datasets/Muskeln/wsis/DKO488_US_wt_9_HE - 2020-04-07 12.03.11.ndpi: Number of tiles that will be kept/all possible tiles: 252/1344\n",
      "/home/Deep_Learner/private/datasets/Muskeln/wsis/DKO476_US_wt_3_HE - 2020-04-07 12.09.33.ndpi: Number of tiles that will be kept/all possible tiles: 232/1120\n",
      "/home/Deep_Learner/private/datasets/Muskeln/wsis/DKO482_US_hom_8_HE - 2020-04-07 11.31.56.ndpi: Number of tiles that will be kept/all possible tiles: 321/2058\n",
      "/home/Deep_Learner/private/datasets/Muskeln/wsis/DKO482_US_hom_9_HE - 2020-04-07 11.33.28.ndpi: Number of tiles that will be kept/all possible tiles: 328/2058\n",
      "/home/Deep_Learner/private/datasets/Muskeln/wsis/DKO476_US_wt_4_HE - 2020-04-07 12.11.11.ndpi: Number of tiles that will be kept/all possible tiles: 98/2597\n",
      "/home/Deep_Learner/private/datasets/Muskeln/wsis/DKO488_US_wt_7_HE - 2020-04-07 11.59.21.ndpi: Number of tiles that will be kept/all possible tiles: 224/2968\n",
      "/home/Deep_Learner/private/datasets/Muskeln/wsis/DKO482_US_hom_5_HE - 2020-04-07 11.27.52.ndpi: Number of tiles that will be kept/all possible tiles: 295/3969\n",
      "/home/Deep_Learner/private/datasets/Muskeln/wsis/DKO482_US_hom_7_HE - 2020-04-07 11.29.54.ndpi: Number of tiles that will be kept/all possible tiles: 286/3969\n",
      "/home/Deep_Learner/private/datasets/Muskeln/wsis/DKO482_US_hom_4_HE - 2020-04-07 11.25.21.ndpi: Number of tiles that will be kept/all possible tiles: 295/3969\n",
      "/home/Deep_Learner/private/datasets/Muskeln/wsis/DKO488_US_wt_8_HE - 2020-04-07 12.01.37.ndpi: Number of tiles that will be kept/all possible tiles: 182/896\n",
      "/home/Deep_Learner/private/datasets/Muskeln/wsis/DKO488_US_wt_10_HE - 2020-04-07 12.04.51.ndpi: Number of tiles that will be kept/all possible tiles: 249/896\n",
      "/home/Deep_Learner/private/datasets/Muskeln/wsis/DKO476_US_wt_2_HE - 2020-04-07 12.08.00.ndpi: Number of tiles that will be kept/all possible tiles: 125/896\n",
      "/home/Deep_Learner/private/datasets/Muskeln/wsis/DKO474_US_hom_5_HE - 2020-04-07 11.56.06.ndpi: Number of tiles that will be kept/all possible tiles: 2029/10017\n",
      "/home/Deep_Learner/private/datasets/Muskeln/wsis/DKO474_US_hom_3_HE - 2020-04-07 11.46.24.ndpi: Number of tiles that will be kept/all possible tiles: 1821/13986\n",
      "/home/Deep_Learner/private/datasets/Muskeln/wsis/DKO474_US_hom_1_HE - 2020-04-07 11.35.13.ndpi: Number of tiles that will be kept/all possible tiles: 1636/17052\n",
      "/home/Deep_Learner/private/datasets/Muskeln/wsis/DKO474_US_hom_2_HE - 2020-04-07 11.40.20.ndpi: Number of tiles that will be kept/all possible tiles: 1601/17094\n",
      "/home/Deep_Learner/private/datasets/Muskeln/wsis/DKO474_US_hom_4_HE - 2020-04-07 11.50.40.ndpi: Number of tiles that will be kept/all possible tiles: 2114/18228\n",
      "/home/Deep_Learner/private/datasets/Muskeln/wsis/DKO476_US_wt_5_HE - 2020-04-07 12.13.27.ndpi: Number of tiles that will be kept/all possible tiles: 500/44247\n",
      "\n",
      "Number of WSIs: 20\n"
     ]
    }
   ],
   "source": [
    "dataset_dicts_muscles = get_nuclei_dicts_muscles(MUSCLES_WSIS.ls(), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### register dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DatasetCatalog.clear()\n",
    "DatasetCatalog.register(dataset_name_muscle, lambda : get_nuclei_dicts_muscles(MUSCLES_WSIS.ls(), None))\n",
    "MetadataCatalog.get(dataset_name_muscle).set(thing_classes=[\"nucleus\"])\n",
    "cfg.DATASETS.TEST = (dataset_name_muscle, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metadata_nuclei_muscle = MetadataCatalog.get(dataset_name_muscle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wsi_name_from_tile_name_muscles(tile_name):\n",
    "    splits = tile_name.split('_')\n",
    "    wsi_name = ''\n",
    "    for i in range(4):\n",
    "        wsi_name += f'{splits[i]}_'\n",
    "    return wsi_name+'HE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DKO476_US_wt_7_HE'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_wsi_name_from_tile_name_muscles(dataset_dicts_muscles[0]['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1dff0d9688947548dd7ad09c9647b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "# all preds saved to one file (gets pretty big)\n",
    "###\n",
    "#dataset_dicts_muscles_with_preds_dir = Path('./dataset_dicts/with_preds')\n",
    "#\n",
    "#if dataset_dicts_muscles_with_preds_pickle_filepath.exists():\n",
    "#    dataset_dicts_muscles = load_pickle(dataset_dicts_muscles_with_preds_pickle_filepath)\n",
    "#\n",
    "#else:\n",
    "#    for d in tqdm(dataset_dicts_muscles[:]):\n",
    "#        img_pil_rgb = open_image_from_wsi(d)\n",
    "#        img_np_bgr = np.array(img_pil_rgb)[:,:,::-1]\n",
    "#        with torch.no_grad():\n",
    "#            pred = predictor(img_np_bgr)\n",
    "#            pred['instances'] = pred['instances'].to('cpu')\n",
    "#        d['prediction'] = pred\n",
    "#        del pred\n",
    "#        torch.cuda.empty_cache()\n",
    "#    save_as_pickle(dataset_dicts_muscles, dataset_dicts_muscles_with_preds_pickle_filepath)\n",
    "\n",
    "\n",
    "###\n",
    "# saved file for every wsi\n",
    "###\n",
    "dataset_dicts_muscles_with_preds_dir = Path('./dataset_dicts/with_preds/')\n",
    "for ts in tqdm(tilesummaries_muscles):\n",
    "    dds_pickle_path = dataset_dicts_muscles_with_preds_dir/f'{ts.wsi_path.stem}.pickle'\n",
    "    if not dds_pickle_path.exists():\n",
    "        print(f'predicting tiles of: {ts.wsi_path.name}')\n",
    "        dds = [d for d in dataset_dicts_muscles if d['tile'].wsi_path == ts.wsi_path]\n",
    "        for d in dds:\n",
    "            img_pil_rgb = open_image_from_wsi(d)\n",
    "            img_np_bgr = np.array(img_pil_rgb)[:,:,::-1]\n",
    "            with torch.no_grad():\n",
    "                pred = predictor(img_np_bgr)\n",
    "                pred['instances'] = pred['instances'].to('cpu')\n",
    "            d['prediction'] = pred\n",
    "            del pred\n",
    "            torch.cuda.empty_cache()\n",
    "        save_as_pickle(dds, dds_pickle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate some nucleus stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# functions per tile\n",
    "###\n",
    "def get_number_of_predicted_instances_one_tile(dataset_dict:Dict)->int:\n",
    "    \"\"\"\n",
    "    returns None, if no prediction exists\n",
    "    \"\"\"\n",
    "    if 'prediction' not in dataset_dict.keys():\n",
    "        #print('This tile has no prediction yet')\n",
    "        return None\n",
    "    return len(dataset_dict['prediction']['instances'])\n",
    "   \n",
    "def get_combined_nucleus_area_one_tile(dataset_dict:Dict)->float:\n",
    "    \"\"\"\n",
    "    result in um²  based on the predictions\n",
    "    returns None, if no prediction exists\n",
    "    \"\"\"\n",
    "    if 'prediction' not in dataset_dict.keys():\n",
    "        #print('This tile has no prediction yet')\n",
    "        return None\n",
    "    n_mask_pixels = dataset_dict['prediction']['instances'].get('pred_masks').int().sum().item()\n",
    "    return n_mask_pixels*slide.get_conversion_factor(dataset_dict['tile'].wsi_path, dataset_dict['tile'].level)**2\n",
    "\n",
    "def get_average_nucleus_area_one_tile(dataset_dict:Dict)->float:\n",
    "    \"\"\"\n",
    "    calculates the mean nucleus size of one tile in um² based on the predictions\n",
    "    returns 0, if there is no nucleus instance in the prediction\n",
    "    returns None, if no prediction exists\n",
    "    Arguments:\n",
    "        dataset_dict: dataset_dict with prediction of one tile\n",
    "    Result:\n",
    "        mean nucleus size in um² \n",
    "        None if  - no prediction at the key 'prediction'\n",
    "                 - no nucleus in the prediction\n",
    "    \"\"\"\n",
    "    if 'prediction' not in dataset_dict.keys():\n",
    "        #print('This tile has no prediction yet')\n",
    "        return None\n",
    "    return 0 if get_number_of_predicted_instances_one_tile(dataset_dict) == 0 else \\\n",
    "        (get_combined_nucleus_area_one_tile(dataset_dict)/get_number_of_predicted_instances_one_tile(dataset_dict))\\\n",
    "        .item()\n",
    "\n",
    "def get_masks_one_tile(dataset_dict:Dict)->torch.tensor:\n",
    "    \"\"\"\n",
    "    return tensor of shape (n_masks, mask_height, mask_width) for all nuclei in the tile\n",
    "    returns None, if there are no masks\n",
    "    \"\"\"\n",
    "    if len(dataset_dict['prediction']['instances']) == 0:\n",
    "        return None\n",
    "    return dataset_dict['prediction']['instances'].get('pred_masks')\n",
    "\n",
    "\n",
    "class Wsi:\n",
    "    tilesummary: tiles.TileSummary = None\n",
    "    dataset_dicts_pickle_path = None\n",
    "    combined_nucleus_area: float = None\n",
    "    average_nucleus_area: float = None\n",
    "    standard_deviation_nucleus_area: float = None\n",
    "    variance_nucleus_area: float = None\n",
    "    \n",
    "    def __init__(self, tilesummary, dataset_dicts_pickle_path):\n",
    "        self.tilesummary = tilesummary\n",
    "        self.dataset_dicts_pickle_path = dataset_dicts_pickle_path\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.tilesummary.wsi_path)\n",
    "    \n",
    "    def free_memory(self):\n",
    "        del self.combined_nucleus_area\n",
    "        del self.average_nucleus_area\n",
    "        del self.standard_deviation_nucleus_area\n",
    "        del self.variance_nucleus_area\n",
    "    \n",
    "    def get_conversion_factor(self):\n",
    "        return slide.get_conversion_factor(self.tilesummary.wsi_path, self.tilesummary.level)\n",
    "    \n",
    "    def get_masks(self)->torch.tensor:\n",
    "        \"\"\"\n",
    "        return tensor of shape (n_masks, mask_height, mask_width) for all nuclei in the wsi \n",
    "        \"\"\"\n",
    "        res = None\n",
    "        for d in load_pickle(self.dataset_dicts_pickle_path):\n",
    "            if get_masks_one_tile(d) == None:\n",
    "                continue\n",
    "            elif res == None:\n",
    "                res = get_masks_one_tile(d)\n",
    "            else:\n",
    "                res = torch.cat([res, get_masks_one_tile(d)])\n",
    "        return res\n",
    "        \n",
    "    def get_number_of_predicted_instances(self)->int:\n",
    "        \"\"\"\n",
    "        returns the number of predicted instances of the whole-slide image based on the predictions \n",
    "        by summing up all predicted instances of all tiles, where predictions exist\n",
    "        tiles with no existing predictions will be ignored silently\n",
    "        \"\"\"\n",
    "        return sum([get_number_of_predicted_instances_one_tile(d) for d in load_pickle(self.dataset_dicts_pickle_path) if \\\n",
    "                    get_number_of_predicted_instances_one_tile(d) != None])\n",
    "    \n",
    "    def get_combined_nucleus_area(self)->float:\n",
    "        \"\"\"\n",
    "        calculates the combined nucleus size of all tiles from one wsi in um² based on the predictions.\n",
    "        tiles with no predictions will be ignored\n",
    "        Arguments:\n",
    "            dataset_dicts: list of all dataset_dicts of the tiles of one whole-slide image\n",
    "        Result:\n",
    "            combined nucleus size in um²\n",
    "        \"\"\"\n",
    "        if self.combined_nucleus_area == None:\n",
    "            if len(load_pickle(self.dataset_dicts_pickle_path)) == 0:\n",
    "                return None\n",
    "            self.combined_nucleus_area = self.get_masks().int().sum()*self.get_conversion_factor()**2\n",
    "        return self.combined_nucleus_area\n",
    "        \n",
    "    def get_average_nucleus_area(self)->float:\n",
    "        \"\"\"\n",
    "        calculates the mean nucleus size from the predictions of all tiles from one wsi in um² based on the predictions\n",
    "        tiles with no predictions will be ignored\n",
    "        Arguments:\n",
    "            dataset_dicts: list of all dataset_dicts of the tiles of one whole-slide image\n",
    "        Result:\n",
    "            mean nucleus size in um²\n",
    "        \"\"\"\n",
    "        if self.average_nucleus_area == None:\n",
    "            area = self.get_combined_nucleus_area()\n",
    "            n = self.get_number_of_predicted_instances()\n",
    "            self.average_nucleus_area = (area/n).item()\n",
    "        return self.average_nucleus_area\n",
    "    \n",
    "    def get_nucleus_area_variance(self)->float:\n",
    "        if self.variance_nucleus_area == None:\n",
    "            self.variance_nucleus_area = np.array(self.get_masks()).sum(axis=(1,2)).var()\\\n",
    "                                            *self.get_conversion_factor()**4\n",
    "        return self.variance_nucleus_area\n",
    "    \n",
    "    def get_nucleus_area_standard_deviation(self)->float:\n",
    "        if self.standard_deviation_nucleus_area == None:\n",
    "            self.standard_deviation_nucleus_area = np.array(self.get_masks()).sum(axis=(1,2)).std()\\\n",
    "                                                    *self.get_conversion_factor()**2\n",
    "        return self.standard_deviation_nucleus_area\n",
    "        \n",
    "       \n",
    "class WsiList:\n",
    "    name = None\n",
    "    wsi_list: List[Wsi] = []\n",
    "    combined_nucleus_area = None\n",
    "    nuclei_areas_numpy = None\n",
    "    average_nucleus_area = None\n",
    "    standard_deviation_nucleus_area = None\n",
    "    variance_nucleus_area = None\n",
    "    \n",
    "    def __init__(self, name:str, wsi_list:List[Wsi]):\n",
    "        self.name = name\n",
    "        self.wsi_list = wsi_list\n",
    "        \n",
    "    def free_memory(self):\n",
    "        del self.combined_nucleus_area\n",
    "        del self.nuclei_areas_numpy\n",
    "        #del self.average_nucleus_area\n",
    "        del self.standard_deviation_nucleus_area\n",
    "        del self.variance_nucleus_area\n",
    "    \n",
    "    def add_wsi(self, wsi:Wsi):\n",
    "        self.wsi_list.append(wsi)\n",
    "        \n",
    "    def get_masks(self)->torch.tensor:\n",
    "        \"\"\"\n",
    "        return tensor of shape (n_masks, mask_height, mask_width) for all nuclei in the wsi\n",
    "        watch out! the masks could be from wsis with different levels of zoom\n",
    "        \"\"\"\n",
    "        res = None\n",
    "        for w in self.wsi_list:\n",
    "            if res is None:\n",
    "                res = w.get_masks()\n",
    "            else:\n",
    "                res = torch.cat([res, w.get_masks()])\n",
    "        return res    \n",
    "    \n",
    "    def get_number_of_predicted_instances(self):\n",
    "        return sum([wsi.get_number_of_predicted_instances() for wsi in self.wsi_list])\n",
    "    \n",
    "    def get_combined_nucleus_area(self):\n",
    "        if self.combined_nucleus_area is None:\n",
    "            self.combined_nucleus_area = sum([wsi.get_combined_nucleus_area() for wsi in self.wsi_list])\n",
    "        return self.combined_nucleus_area\n",
    "    \n",
    "    def get_average_nucleus_area(self):\n",
    "        return self.get_combined_nucleus_area()/self.get_number_of_predicted_instances()\n",
    "    \n",
    "    def get_nuclei_areas(self):\n",
    "        \"\"\"\n",
    "        returns numpy array of shape (number_of_nuclei, nucleus_area_um²)\n",
    "        different zoom levels of the wsis get taken into account\n",
    "        \"\"\"\n",
    "        if self.nuclei_areas_numpy is None:\n",
    "            nuclei_areas = None\n",
    "            for w in self.wsi_list:\n",
    "                nas_w = np.array(w.get_masks()).sum(axis=(1,2))*w.get_conversion_factor()**2 \n",
    "                if nuclei_areas is None:\n",
    "                    nuclei_areas = nas_w\n",
    "                else: \n",
    "                    nuclei_areas = np.concatenate((nuclei_areas, nas_w))\n",
    "            self.nuclei_areas_numpy = nuclei_areas\n",
    "        return self.nuclei_areas_numpy\n",
    "    \n",
    "    def get_nucleus_area_variance(self)->float:\n",
    "        if self.variance_nucleus_area is None:\n",
    "            self.variance_nucleus_area = self.get_nuclei_areas().var()\n",
    "        return self.variance_nucleus_area\n",
    "            \n",
    "    \n",
    "    def get_nucleus_area_standard_deviation(self)->float:\n",
    "        if self.standard_deviation_nucleus_area == None:\n",
    "            self.standard_deviation_nucleus_area = self.get_nuclei_areas().std()\n",
    "        return self.standard_deviation_nucleus_area\n",
    "    \n",
    "    def plot_area_distribution_histogram(self):\n",
    "        data = []\n",
    "        for wsi in self.wsi_list:\n",
    "            for d in load_pickle(wsi.dataset_dicts_pickle_path):\n",
    "                for m in d['prediction']['instances'].get('pred_masks'):\n",
    "                    data.append(m.int().sum()*slide.get_conversion_factor(d['tile'].wsi_path, d['tile'].level)**2)\n",
    "            wsi.free_memory()\n",
    "        bins = np.linspace(0, max(data), 200)\n",
    "        plt.hist(probs_true_positive, bins, alpha=1.0, label=None)\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.title(label=name)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.xlabel('area in um²')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "### init two WsiList objects\n",
    "\n",
    "wsi_list_hom = WsiList('hom', [])\n",
    "wsi_list_wt = WsiList('wt', [])\n",
    "\n",
    "for ts in tilesummaries_muscles:\n",
    "    dds = [d for d in dataset_dicts_muscles if d['tile'].wsi_path == ts.wsi_path]\n",
    "    if 'hom' in ts.wsi_path.name:\n",
    "        wsi_list_hom.add_wsi(Wsi(ts, dataset_dicts_muscles_with_preds_dir/f'{ts.wsi_path.stem}.pickle'))\n",
    "    elif 'wt' in ts.wsi_path.name:\n",
    "        wsi_list_wt.add_wsi(Wsi(ts, dataset_dicts_muscles_with_preds_dir/f'{ts.wsi_path.stem}.pickle'))\n",
    "    else:\n",
    "        assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ed28d56fa754934ad55c4e5df6b6c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28862b0c383549d1af895db0000b0fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### function that shall be run in parallel\n",
    "def create_df_nuclei_stats_muscles_single_wsis(wsi):\n",
    "    df = pd.DataFrame(columns=['wsi name', 'nuclei', 'avg nucleus area um²', 'standard deviation', 'variance'])\n",
    "    name = wsi.tilesummary.wsi_path.stem\n",
    "    n = wsi.get_number_of_predicted_instances()\n",
    "    avg_area = wsi.get_average_nucleus_area()\n",
    "    sd = wsi.get_nucleus_area_standard_deviation()\n",
    "    var = wsi.get_nucleus_area_variance()\n",
    "    df = df.append({'wsi name':name, \n",
    "                    'nuclei':n, \n",
    "                    'avg nucleus area um²':avg_area,\n",
    "                    'standard deviation':sd, \n",
    "                    'variance':var}, \n",
    "                    ignore_index=True)\n",
    "    wsi.free_memory()\n",
    "    return df\n",
    "\n",
    "\n",
    "###sequential execution\n",
    "results = []\n",
    "for wsi in tqdm(wsi_list_wt.wsi_list+wsi_list_hom.wsi_list):\n",
    "    results.append(create_df_nuclei_stats_muscles_single_wsis(wsi))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###multithread execution\n",
    "#pbar = tqdm(total=len(wsi_list_wt.wsi_list+wsi_list_hom.wsi_list))\n",
    "#results = []\n",
    "#def update(res):\n",
    "#    results.append(res)\n",
    "#    pbar.update()\n",
    "#     \n",
    "#with multiprocessing.Pool(processes=4) as pool:\n",
    "#    for wsi in wsi_list_wt.wsi_list+wsi_list_hom.wsi_list:\n",
    "#        pool.apply_async(create_df_nuclei_stats_muscles_single_wsis, \n",
    "#                         args=(wsi), \n",
    "#                               callback=update)\n",
    "#        \n",
    "#            \n",
    "#    pool.close()\n",
    "#    pool.join()\n",
    "\n",
    "### merge results into one dataframe\n",
    "merged_df = None\n",
    "for res in tqdm(results):\n",
    "    if merged_df is None:\n",
    "        merged_df = res\n",
    "    else:\n",
    "        merged_df = merged_df.append(res, sort=False)\n",
    "\n",
    "### save end result to disk\n",
    "df_nuclei_stats_muscles_single_wsis = merged_df.drop_duplicates(inplace=False)\n",
    "df_nuclei_stats_muscles_single_wsis.to_csv(Path('./single_wsi_stats.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wsi name</th>\n",
       "      <th>nuclei</th>\n",
       "      <th>avg nucleus area um²</th>\n",
       "      <th>standard deviation</th>\n",
       "      <th>variance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DKO476_US_wt_7_HE - 2020-04-07 12.21.42</td>\n",
       "      <td>2423</td>\n",
       "      <td>22.903921</td>\n",
       "      <td>24.126084</td>\n",
       "      <td>582.067933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DKO488_US_wt_12_HE - 2020-04-07 12.06.24</td>\n",
       "      <td>1983</td>\n",
       "      <td>23.685217</td>\n",
       "      <td>29.154208</td>\n",
       "      <td>849.967824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DKO488_US_wt_9_HE - 2020-04-07 12.03.11</td>\n",
       "      <td>2742</td>\n",
       "      <td>21.556290</td>\n",
       "      <td>21.622521</td>\n",
       "      <td>467.533401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DKO476_US_wt_3_HE - 2020-04-07 12.09.33</td>\n",
       "      <td>2235</td>\n",
       "      <td>21.922083</td>\n",
       "      <td>20.310072</td>\n",
       "      <td>412.499044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DKO476_US_wt_4_HE - 2020-04-07 12.11.11</td>\n",
       "      <td>641</td>\n",
       "      <td>33.826317</td>\n",
       "      <td>42.324942</td>\n",
       "      <td>1791.400695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DKO488_US_wt_7_HE - 2020-04-07 11.59.21</td>\n",
       "      <td>2341</td>\n",
       "      <td>24.981358</td>\n",
       "      <td>27.398928</td>\n",
       "      <td>750.701241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DKO488_US_wt_8_HE - 2020-04-07 12.01.37</td>\n",
       "      <td>1702</td>\n",
       "      <td>26.889074</td>\n",
       "      <td>29.099665</td>\n",
       "      <td>846.790518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DKO488_US_wt_10_HE - 2020-04-07 12.04.51</td>\n",
       "      <td>2765</td>\n",
       "      <td>21.932411</td>\n",
       "      <td>22.059789</td>\n",
       "      <td>486.634309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DKO476_US_wt_2_HE - 2020-04-07 12.08.00</td>\n",
       "      <td>985</td>\n",
       "      <td>25.354111</td>\n",
       "      <td>31.267081</td>\n",
       "      <td>977.630380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DKO476_US_wt_5_HE - 2020-04-07 12.13.27</td>\n",
       "      <td>2040</td>\n",
       "      <td>24.459785</td>\n",
       "      <td>27.770803</td>\n",
       "      <td>771.217487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DKO482_US_hom_8_HE - 2020-04-07 11.31.56</td>\n",
       "      <td>3021</td>\n",
       "      <td>23.060713</td>\n",
       "      <td>22.279321</td>\n",
       "      <td>496.368127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DKO482_US_hom_9_HE - 2020-04-07 11.33.28</td>\n",
       "      <td>3228</td>\n",
       "      <td>22.414019</td>\n",
       "      <td>20.669557</td>\n",
       "      <td>427.230606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DKO482_US_hom_5_HE - 2020-04-07 11.27.52</td>\n",
       "      <td>2844</td>\n",
       "      <td>21.724112</td>\n",
       "      <td>21.407406</td>\n",
       "      <td>458.277032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DKO482_US_hom_7_HE - 2020-04-07 11.29.54</td>\n",
       "      <td>2578</td>\n",
       "      <td>20.277508</td>\n",
       "      <td>18.109222</td>\n",
       "      <td>327.943937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DKO482_US_hom_4_HE - 2020-04-07 11.25.21</td>\n",
       "      <td>2795</td>\n",
       "      <td>22.205652</td>\n",
       "      <td>21.088077</td>\n",
       "      <td>444.706974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DKO474_US_hom_5_HE - 2020-04-07 11.56.06</td>\n",
       "      <td>23100</td>\n",
       "      <td>20.894659</td>\n",
       "      <td>20.450132</td>\n",
       "      <td>418.207905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DKO474_US_hom_3_HE - 2020-04-07 11.46.24</td>\n",
       "      <td>23531</td>\n",
       "      <td>21.153095</td>\n",
       "      <td>20.618318</td>\n",
       "      <td>425.115050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DKO474_US_hom_1_HE - 2020-04-07 11.35.13</td>\n",
       "      <td>19765</td>\n",
       "      <td>21.540108</td>\n",
       "      <td>20.503980</td>\n",
       "      <td>420.413212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DKO474_US_hom_2_HE - 2020-04-07 11.40.20</td>\n",
       "      <td>19980</td>\n",
       "      <td>21.127161</td>\n",
       "      <td>22.051032</td>\n",
       "      <td>486.247993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DKO474_US_hom_4_HE - 2020-04-07 11.50.40</td>\n",
       "      <td>25122</td>\n",
       "      <td>20.933447</td>\n",
       "      <td>21.440580</td>\n",
       "      <td>459.698472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   wsi name nuclei  avg nucleus area um²  \\\n",
       "0   DKO476_US_wt_7_HE - 2020-04-07 12.21.42   2423             22.903921   \n",
       "0  DKO488_US_wt_12_HE - 2020-04-07 12.06.24   1983             23.685217   \n",
       "0   DKO488_US_wt_9_HE - 2020-04-07 12.03.11   2742             21.556290   \n",
       "0   DKO476_US_wt_3_HE - 2020-04-07 12.09.33   2235             21.922083   \n",
       "0   DKO476_US_wt_4_HE - 2020-04-07 12.11.11    641             33.826317   \n",
       "0   DKO488_US_wt_7_HE - 2020-04-07 11.59.21   2341             24.981358   \n",
       "0   DKO488_US_wt_8_HE - 2020-04-07 12.01.37   1702             26.889074   \n",
       "0  DKO488_US_wt_10_HE - 2020-04-07 12.04.51   2765             21.932411   \n",
       "0   DKO476_US_wt_2_HE - 2020-04-07 12.08.00    985             25.354111   \n",
       "0   DKO476_US_wt_5_HE - 2020-04-07 12.13.27   2040             24.459785   \n",
       "0  DKO482_US_hom_8_HE - 2020-04-07 11.31.56   3021             23.060713   \n",
       "0  DKO482_US_hom_9_HE - 2020-04-07 11.33.28   3228             22.414019   \n",
       "0  DKO482_US_hom_5_HE - 2020-04-07 11.27.52   2844             21.724112   \n",
       "0  DKO482_US_hom_7_HE - 2020-04-07 11.29.54   2578             20.277508   \n",
       "0  DKO482_US_hom_4_HE - 2020-04-07 11.25.21   2795             22.205652   \n",
       "0  DKO474_US_hom_5_HE - 2020-04-07 11.56.06  23100             20.894659   \n",
       "0  DKO474_US_hom_3_HE - 2020-04-07 11.46.24  23531             21.153095   \n",
       "0  DKO474_US_hom_1_HE - 2020-04-07 11.35.13  19765             21.540108   \n",
       "0  DKO474_US_hom_2_HE - 2020-04-07 11.40.20  19980             21.127161   \n",
       "0  DKO474_US_hom_4_HE - 2020-04-07 11.50.40  25122             20.933447   \n",
       "\n",
       "   standard deviation     variance  \n",
       "0           24.126084   582.067933  \n",
       "0           29.154208   849.967824  \n",
       "0           21.622521   467.533401  \n",
       "0           20.310072   412.499044  \n",
       "0           42.324942  1791.400695  \n",
       "0           27.398928   750.701241  \n",
       "0           29.099665   846.790518  \n",
       "0           22.059789   486.634309  \n",
       "0           31.267081   977.630380  \n",
       "0           27.770803   771.217487  \n",
       "0           22.279321   496.368127  \n",
       "0           20.669557   427.230606  \n",
       "0           21.407406   458.277032  \n",
       "0           18.109222   327.943937  \n",
       "0           21.088077   444.706974  \n",
       "0           20.450132   418.207905  \n",
       "0           20.618318   425.115050  \n",
       "0           20.503980   420.413212  \n",
       "0           22.051032   486.247993  \n",
       "0           21.440580   459.698472  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nuclei_stats_muscles_single_wsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3922cddbd334db6bc034d29d06aa986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### function that shall be run in parallel\n",
    "def create_df_nuclei_stats_muscles_wsi_lists(wsi_list):\n",
    "    df = pd.DataFrame(columns=['list name', 'nuclei', 'avg nucleus area um²', 'standard deviation', 'variance'])\n",
    "    name = wsi_list.name\n",
    "    n = wsi_list.get_number_of_predicted_instances()\n",
    "    avg_area = wsi_list.get_average_nucleus_area()\n",
    "    sd = wsi_list.get_nucleus_area_standard_deviation()\n",
    "    var = wsi_list.get_nucleus_area_variance()\n",
    "    df = df.append({'list name':name, \n",
    "                    'nuclei':n, \n",
    "                    'avg nucleus area um²':avg_area,\n",
    "                    'standard deviation':sd, \n",
    "                    'variance':var}, \n",
    "                    ignore_index=True)\n",
    "    wsi_list.free_memory()\n",
    "    return df\n",
    "\n",
    "\n",
    "###sequential execution\n",
    "results = []\n",
    "for wsi_list in tqdm([wsi_list_wt, wsi_list_hom]):\n",
    "    results.append(create_df_nuclei_stats_muscles_wsi_lists(wsi_list))\n",
    "\n",
    "\n",
    "###multithread execution\n",
    "#pbar = tqdm(total=len(wsi_list_wt.wsi_list+wsi_list_hom.wsi_list))\n",
    "#results = []\n",
    "#def update(res):\n",
    "#    results.append(res)\n",
    "#    pbar.update()\n",
    "#     \n",
    "#with multiprocessing.Pool(4) as pool:\n",
    "#    for wsi_list in [wsi_list_wt, wsi_list_hom]:\n",
    "#        pool.apply_async(create_df_nuclei_stats_muscles_wsi_lists, \n",
    "#                         args=(wsi_list), \n",
    "#                               callback=update)\n",
    "#        \n",
    "#            \n",
    "#    pool.close()\n",
    "#    pool.join()\n",
    "\n",
    "### merge results into one dataframe\n",
    "merged_df = None\n",
    "for res in tqdm(results):\n",
    "    if merged_df is None:\n",
    "        merged_df = res\n",
    "    else:\n",
    "        merged_df = merged_df.append(res, sort=False)\n",
    "        \n",
    "df_nuclei_stats_muscles_wsi_lists = merged_df\n",
    "df_nuclei_stats_muscles_wsi_lists.to_csv(Path('./wsi_lists_stats.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsi_list_hom.plot_area_distribution_histogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wsi_list_wt.plot_area_distribution_histogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize(dataset_dicts=dataset_dicts_muscles[1:20], \n",
    "          metadata=metadata_nuclei_muscle,\n",
    "          ground_truth_available=False,\n",
    "          with_preds=True,\n",
    "          predictor=predictor, \n",
    "          extract_otf_from_wsi=True,  \n",
    "          captions=None, \n",
    "          figsize=(10,10),\n",
    "          caption_generators=[get_combined_nucleus_area_one_tile, get_average_nucleus_area_one_tile])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# keep for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#DKO474_US_hom_1_HE - 2020-04-07 11.35.13.ndpi\n",
    "# meassured by hand in case viewer app\n",
    "##width in um\n",
    "22916.1\n",
    "\n",
    "##height in um\n",
    "9482.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLM Py3",
   "language": "python",
   "name": "dlm_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
