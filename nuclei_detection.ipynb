{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies\n",
    "!pip install --upgrade pip\n",
    "!pip install -U torch==1.4+cu100 torchvision==0.5+cu100 -f https://download.pytorch.org/whl/torch_stable.html \n",
    "!pip install cython pyyaml==5.1\n",
    "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
    "import torch, torchvision\n",
    "torch.__version__\n",
    "torchvision.__version__\n",
    "!gcc --version\n",
    "\n",
    "# install detectron2:\n",
    "!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu100/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "from bokeh.io import output_notebook\n",
    "from typing import Dict, List, Callable\n",
    "import os\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "Path.ls = lambda x: [p for p in list(x.iterdir()) if '.ipynb_checkpoints' not in p.name]\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import sys\n",
    "sys.path.append('../preprocessing_pipeline/python-wsi-preprocessing/')\n",
    "import wsi\n",
    "from wsi import tiles, util\n",
    "import fastai\n",
    "from fastai import vision\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches\n",
    "import pycocotools\n",
    "from pycocotools import mask\n",
    "from itertools import groupby\n",
    "\n",
    " \n",
    "PATH = Path('/home/Deep_Learner/shared/Datasets/2018_Data_Science_Bowl/')\n",
    "STAGE1_TRAIN = PATH/'stage1_train'\n",
    "STAGE1_TEST = PATH/'stage1_test'\n",
    "STAGE1_TRAIN_LABELS = PATH/'stage1_train_labels.csv'\n",
    "STAGE1_TEST_LABELS = PATH/'stage1_solution.csv'\n",
    "\n",
    "seed = 19\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path_from_id(img_id:str, base_path:pathlib.Path)->pathlib.Path:\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        base_path: here ~/2018_Data_Science_Bowl/stage1_train/   or   ~/2018_Data_Science_Bowl/stage1_test/\n",
    "    \"\"\"\n",
    "    for p in base_path.ls():\n",
    "        if img_id in p.stem:\n",
    "            return p\n",
    "\n",
    "def get_masks_path_from_id(img_id:str, base_path:pathlib.Path)->pathlib.Path:\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        base_path: here ~/2018_Data_Science_Bowl/stage1_train/   or   ~/2018_Data_Science_Bowl/stage1_test/\n",
    "    \"\"\"\n",
    "    return get_path_from_id(img_id, base_path)/'masks'\n",
    "        \n",
    "def open_mask_as_np(path:pathlib.Path)->numpy.ndarray:\n",
    "    return np.asarray(PIL.Image.open(path), dtype=np.bool)\n",
    "\n",
    "def show_np(arr:numpy.ndarray):\n",
    "    plt.imshow(arr)\n",
    "    plt.show()\n",
    "\n",
    "def show_np_with_bboxes(img:numpy.ndarray, bboxes:List[numpy.ndarray]):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        img: img as numpy array\n",
    "        bboxes: List of bounding boxes where each bbox is a numpy array: \n",
    "                array([ x-upper-left, y-upper-left,  width,  height]) \n",
    "                e.g. array([ 50., 211.,  17.,  19.])\n",
    "    \"\"\"    \n",
    "    # Create figure and axes\n",
    "    fig,ax = plt.subplots(1)    \n",
    "    # Display the image\n",
    "    ax.imshow(img)    \n",
    "    # Create a Rectangle patch for each bbox\n",
    "    for b in bboxes:\n",
    "        rect = matplotlib.patches.Rectangle((b[0],b[1]),b[2],b[3],linewidth=1,edgecolor='r',facecolor='none')    \n",
    "        # Add the patch to the Axes\n",
    "        ax.add_patch(rect)    \n",
    "    plt.show()    \n",
    "\n",
    "def merge_masks(masks:List[numpy.ndarray], shape:tuple=(256,256))->numpy.ndarray:\n",
    "    merged_mask = np.zeros((shape[0],shape[1]), dtype=np.bool)\n",
    "    for mask in masks:\n",
    "        merged_mask = np.maximum(merged_mask, mask)\n",
    "    return merged_mask\n",
    "\n",
    "def merge_masks_from_path(path:pathlib.Path, shape:tuple=(256,256))->numpy.ndarray:\n",
    "    masks = [open_mask_as_np(p) for p in path.ls()]\n",
    "    return merge_masks(masks, shape)\n",
    "\n",
    "\n",
    "def rle_encode(mask_np:numpy.ndarray)->List[int]:\n",
    "    '''\n",
    "    mask_np: numpy array of shape (height, width), 1 = mask, 0 = background\n",
    "    Returns run length as list\n",
    "    '''\n",
    "    dots = np.where(mask_np.T.flatten()==1)[0] # .T sets Fortran order down-then-right\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b>prev+1): run_lengths.extend((b+1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "def rle_from_list_to_string(rle_as_list:List[int])->str:\n",
    "    return ' '.join([str(e) for e in rle_as_list])\n",
    "\n",
    "def rle_decode(mask_rle:str, shape:tuple=(256, 256))->numpy.ndarray:\n",
    "    '''\n",
    "    Arguments:\n",
    "        mask_rle: run-length as string formated (start length) e.g. \"6908 1 7161 8 7417 8\"\n",
    "        shape: (height,width) of array to return\n",
    "    Returns: \n",
    "        numpy array, True == mask, False == background\n",
    "    '''\n",
    "    #print('rle_decode(mask_rle = ', mask_rle)\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    mask = np.zeros(shape[0]*shape[1], dtype=np.bool)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        mask[lo:hi] = True\n",
    "    # Needed to align to RLE direction\n",
    "    return mask.reshape(shape).T\n",
    "\n",
    "def get_mask_list_from_rle_for_one_id(img_id:str, \n",
    "                                      df:pandas.DataFrame, \n",
    "                                      coloumn_name_ids:str='ImageId',\n",
    "                                      coloumn_name_rle:str='EncodedPixels',\n",
    "                                      shape:tuple=(256,256))->List[numpy.ndarray]:\n",
    "    masks_rle_df = df.loc[df[coloumn_name_ids] == img_id]\n",
    "    masks = []\n",
    "    for index, row in masks_rle_df.iterrows():\n",
    "        #print(row[coloumn_name_rle])\n",
    "        masks.append(rle_decode(row[coloumn_name_rle], shape))\n",
    "    return masks\n",
    "\n",
    "def mask_to_bbox(mask_np:numpy.ndarray)->numpy.ndarray:\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        mask_np: binary mask as numpy array where mask == True or 1 or 1.0 and background == False or 0 or 0.0\n",
    "    Returns:\n",
    "        bounding box as numpy array: array([ x-upper-left, y-upper-left,  width,  height]) \n",
    "                                        e.g. array([ 50., 211.,  17.,  19.])\n",
    "    \"\"\"\n",
    "    return pycocotools.mask.toBbox(pycocotools.mask.encode(np.asarray(mask_np, order=\"F\")))\n",
    "\n",
    "\n",
    "def rle_encode_pycoco(mask_np:numpy.ndarray)->dict:\n",
    "    '''\n",
    "    Arguments:\n",
    "        mask_np: numpy array of shape (height, width), 1 = mask, 0 = background\n",
    "    Returns: \n",
    "        dict with size and per-pixel segmentation mask in COCO's RLE format.        \n",
    "    '''\n",
    "    #option 1\n",
    "    return pycocotools.mask.encode(np.asarray(mask, order=\"F\"))\n",
    "    #option 2\n",
    "    #rle = {'counts': [], 'size': list(mask_np.shape)}\n",
    "    #counts = rle.get('counts')\n",
    "    #for i, (value, elements) in enumerate(groupby(mask_np.ravel(order='F'))):\n",
    "    #    if i == 0 and value == 1:\n",
    "    #        counts.append(0)\n",
    "    #    counts.append(len(list(elements)))\n",
    "    #return rle\n",
    "\n",
    "#!pip install imantics\n",
    "from imantics import Polygons, Mask\n",
    "def get_polygon_from_binary_mask(mask_np:numpy.ndarray)->List[int]:\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        binary mask as numpy array\n",
    "    Returns:\n",
    "        list[int] is one simple polygon in the format of [x1, y1, ..., xn, yn]\n",
    "    \"\"\"\n",
    "    return list(Mask(mask_np).polygons()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_trn_lbs_df = pd.read_csv(STAGE1_TRAIN_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_trn_lbs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = s1_trn_lbs_df.iloc[0][0];img_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge existing single masks of one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id_p = get_path_from_id(img_id, STAGE1_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_path = get_masks_path_from_id(img_id, STAGE1_TRAIN);masks_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_np = [open_mask_as_np(p) for p in masks_path.ls()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_mask = merge_masks_from_path(masks_path,(256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_np(merged_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## decode run length encoding to mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = get_mask_list_from_rle_for_one_id(img_id, s1_trn_lbs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_mask = merge_masks(masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_np(merged_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get bounding box from mask in format x,y (left upper corner); width, heigth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_p = get_path_from_id(img_id, STAGE1_TRAIN);img_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks_p = get_masks_path_from_id(img_id, STAGE1_TRAIN);masks_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes = []\n",
    "for p in masks_p.ls():\n",
    "    mask_np = open_mask_as_np(p)\n",
    "    bboxes.append(mask_to_bbox(mask_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_mask = merge_masks_from_path(masks_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_np_with_bboxes(merged_mask,bboxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom dataset for detectron2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example custom dataset balloons (https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5#scrollTo=4Qg7zSVOulkb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to restart your runtime prior to this, to let your installation take effect\n",
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from detectron2.structures import BoxMode\n",
    "\n",
    "def get_balloon_dicts(img_dir):\n",
    "    json_file = os.path.join(img_dir, \"via_region_data.json\")\n",
    "    with open(json_file) as f:\n",
    "        imgs_anns = json.load(f)\n",
    "\n",
    "    dataset_dicts = []\n",
    "    for idx, v in enumerate(imgs_anns.values()):\n",
    "        record = {}\n",
    "        \n",
    "        filename = os.path.join(img_dir, v[\"filename\"])\n",
    "        height, width = cv2.imread(filename).shape[:2]\n",
    "        \n",
    "        record[\"file_name\"] = filename\n",
    "        record[\"image_id\"] = idx\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "      \n",
    "        annos = v[\"regions\"]\n",
    "        objs = []\n",
    "        for _, anno in annos.items():\n",
    "            assert not anno[\"region_attributes\"]\n",
    "            anno = anno[\"shape_attributes\"]\n",
    "            px = anno[\"all_points_x\"]\n",
    "            py = anno[\"all_points_y\"]\n",
    "            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "            poly = [p for x in poly for p in x]\n",
    "\n",
    "            obj = {\n",
    "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                \"segmentation\": [poly],\n",
    "                \"category_id\": 0,\n",
    "                \"iscrowd\": 0\n",
    "            }\n",
    "            objs.append(obj)\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts\n",
    "\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "for d in [\"train\", \"val\"]:\n",
    "    DatasetCatalog.register(\"balloon_\" + d, lambda d=d: get_balloon_dicts(\"balloon/\" + d))\n",
    "    MetadataCatalog.get(\"balloon_\" + d).set(thing_classes=[\"balloon\"])\n",
    "balloon_metadata = MetadataCatalog.get(\"balloon_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_dicts_balloons = get_balloon_dicts(\"./balloon/train\")\n",
    "for d in random.sample(dataset_dicts_balloons, 3):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=balloon_metadata, scale=0.5)\n",
    "    vis = visualizer.draw_dataset_dict(d)\n",
    "    #cv2.imshow(vis.get_image()[:, :, ::-1])\n",
    "    show_np(vis.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset for nuclei detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to restart your runtime prior to this, to let your installation take effect\n",
    "# Some basic setup:\n",
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.structures import BoxMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rle_labels = pd.concat([pd.read_csv(STAGE1_TRAIN_LABELS), \n",
    "                             pd.read_csv(STAGE1_TEST_LABELS)], \n",
    "                            ignore_index=True, sort=False)\n",
    "\n",
    "def get_nuclei_dicts(dataset_path):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        dataset_path: STAGE1_TRAIN or STAGE1_TEST\n",
    "    \"\"\"\n",
    "\n",
    "    img_paths = [path/'images'/f'{path.name}.png' for path in dataset_path.ls()]\n",
    "    \n",
    "    dataset_dicts = []\n",
    "    for path in tqdm(train_paths):\n",
    "        record = {}\n",
    "        \n",
    "        img_pil = PIL.Image.open(path)         \n",
    "        width = img_pil.width\n",
    "        height = img_pil.height\n",
    "        \n",
    "        record[\"file_name\"] = path\n",
    "        record[\"image_id\"] = path.stem\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "      \n",
    "        masks = get_mask_list_from_rle_for_one_id(path.stem, df_rle_labels, shape=(height,width))\n",
    "        objs = []\n",
    "        for mask in masks:\n",
    "            obj = {\n",
    "                \"bbox\": list(mask_to_bbox(mask)),\n",
    "                \"bbox_mode\": BoxMode.XYWH_ABS,\n",
    "                \"segmentation\": [get_polygon_from_binary_mask(mask)],\n",
    "                #\"segmentation\": rle_encode_pycoco(mask),\n",
    "                \"category_id\": 0,\n",
    "                \"iscrowd\": 0\n",
    "            }\n",
    "            objs.append(obj)\n",
    "            \n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "for d in [STAGE1_TRAIN, STAGE1_TEST]:\n",
    "    DatasetCatalog.register(\"nuclei_\" + d.name, lambda d=d: get_balloon_dicts(d))\n",
    "    MetadataCatalog.get(f\"nuclei_{d.name}\").set(thing_classes=[\"nucleus\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuclei_metadata = MetadataCatalog.get(f\"nuclei_{STAGE1_TRAIN.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_dicts_nuclei = get_nuclei_dicts(STAGE1_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for d in random.sample(dataset_dicts_nuclei, 3):\n",
    "    #print(d[\"file_name\"])\n",
    "    img = cv2.imread(str(d[\"file_name\"]))\n",
    "    #img = np.asarray(Image.open(p))\n",
    "    visualizer = Visualizer(img[:, :, ::-1], metadata=nuclei_metadata, scale=0.5)\n",
    "    vis = visualizer.draw_dataset_dict(d)\n",
    "    #cv2.imshow(vis.get_image()[:, :, ::-1])\n",
    "    show_np(vis.get_image()[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dicts_nuclei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLM Py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
